{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Input Vars]\n",
    "#   1. <string> PATH_TO_DESIRED_LOCATION: It should be the directory containing (1) images/ (2) train.txt (3) test.txt (4) val.txt\n",
    "\n",
    "# [Output Vars]\n",
    "#   1. <ndarray> np_train_txt: It contains both the directory to a specific image and the related label.\n",
    "#   2. <ndarray> np_test_txt: It contains both the directory to a specific image and the related label.\n",
    "#   3. <ndarray> np_val_txt: It contains both the directory to a specific image and the related label.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def read_metadata_files(PATH_TO_DESIRED_LOCATION):\n",
    "    # train.txt\n",
    "    train_txt = pd.read_csv(PATH_TO_DESIRED_LOCATION+\"train.txt\", sep=\" \")\n",
    "    NP_TRAIN_TXT = np.array(train_txt)\n",
    "    \n",
    "    # test.txt\n",
    "    test_txt = pd.read_csv(PATH_TO_DESIRED_LOCATION+\"test.txt\", sep=\" \")\n",
    "    NP_TEST_TXT = np.array(test_txt)\n",
    "    \n",
    "    # val.txt\n",
    "    val_txt = pd.read_csv(PATH_TO_DESIRED_LOCATION+\"val.txt\", sep=\" \")\n",
    "    NP_VAL_TXT = np.array(val_txt)\n",
    "    \n",
    "    print(f\"[Check] There are {NP_TRAIN_TXT.shape[0]} pairs in train.txt.\")\n",
    "    print(f\"[Check] There are {NP_TEST_TXT.shape[0]} pairs in test.txt.\")\n",
    "    print(f\"[Check] There are {NP_VAL_TXT.shape[0]} pairs in val.txt.\\n\")\n",
    "    \n",
    "    return NP_TRAIN_TXT, NP_TEST_TXT, NP_VAL_TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Zero Padding #\n",
    "################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> X: Unpadded image. The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <int> pad: expected number of pads on each side. The shape is (n_H_prev + 2 * pad, n_W_prev + 2 * pad, n_C_prev).\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> X_pad: Padded image.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __zero_pad(X, pad):\n",
    "    X_pad = np.pad(X, ((pad, pad), (pad, pad),(0,0)), \"constant\", constant_values = 0)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Conv Single Step #\n",
    "####################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> a_slice_prev: slice of previous feature maps. The shape is (f, f, n_C_prev).\n",
    "#  2. <ndarray> K: A single weight matrix (kernel). The shape is (f, f, n_C_prev).\n",
    "#  3. <ndarray> b: A single bias term. The shape is (1, 1, 1).\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <float> Z: a scalar derived from convolution operation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __conv_single_step(s_slice, K, b):\n",
    "    \n",
    "    S = np.multiply(s_slice, K)\n",
    "    Z = np.sum(S)\n",
    "    Z = Z + float(b)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Conv Forward Propagation #\n",
    "############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> S_prev: The previous feature maps (after activation and pooling). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <ndarray> K: Kernels in a layer. The shape is (f, f, n_C_prev, n_C).\n",
    "#  3. <ndarray> b: biases in a layer. THe shape is (1, 1, 1, n_C).\n",
    "#  4. <dictionary> hparam: this contains hyper parameters like \"pad\" and \"stride\".\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> C: This would be the feature map in the next layer (but before activation). The shape is (n_H, n_W, n_C).\n",
    "#  2. <dictionary> cache: Cache the values needed for backward propagation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def conv_forward(S_prev, K, b, hparam):\n",
    "    \n",
    "    # 1. Retrieve shape of A_prev. We need this to compute the shape of the feature map in the next layer.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = S_prev.shape\n",
    "    \n",
    "    # 2. Retrieve shape of K. We also need this (i.e. f) to compute the shape of the feature map in the next layer.\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "    \n",
    "    # 3. Retrieve info. from hyper parameters. We need them to compute the shape of the feature map in the next layer, too.\n",
    "    stride = hparam[\"stride\"]\n",
    "    pad = hparam[\"pad\"]\n",
    "    \n",
    "    # 4. With info from 1. ~ 3., we can compute the dimension for the feature map in the next layer.\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # 5. Initialize feature maps in the next layer with zeros. Note #Kernel is equal to #Channel of the feature map.\n",
    "    C = np.zeros((n_H, n_W, n_C))\n",
    "    \n",
    "    # 6. Pad S_prev\n",
    "    S_prev_pad = __zero_pad(S_prev, pad)\n",
    "    \n",
    "    # 7. Do Cross-Relation Operation. Note the shape of the output feature map would be (n_H, n_W, n_C).\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                \n",
    "                # Define the corners in the S_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Get the slice.\n",
    "                S_prev_slice = S_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :]\n",
    "                \n",
    "                # Feed it into __conv_single_step(a_slice, K, b). Note we use one kernel and one bias term at once.\n",
    "                C[h, w, c] = __conv_single_step(S_prev_slice, K[:,:,:,c], b[:,:,:,c])\n",
    "    \n",
    "    # 8. Check if the output feature map have the valid shape.\n",
    "    assert(C.shape == (n_H, n_W, n_C))\n",
    "    \n",
    "    # 9. Store the cache for backward propagation\n",
    "    cache = (S_prev, K, b, hparam)\n",
    "    \n",
    "    return C, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Pool Forward Propagation #\n",
    "############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> A_prev: The previous feature maps (after activation). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <dictionary> hparam: It contains \"f\" and \"stride\".\n",
    "#  3. <string> mode: Switch between \"maxpooling\" and \"avgpooling\". The shape is (n_H, n_W, n_C). (n_C = n_C_prev)\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> S: The output feature map after pooling operation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pool_forward(A_prev, hparam, mode = \"maxpooling\"):\n",
    "    # 1. Retrieve shape of A_prev.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # 2. Retrieve info from hyper parameter\n",
    "    f = hparam[\"f\"]\n",
    "    stride = hparam[\"stride\"]\n",
    "\n",
    "    # 3. Define the shape of output of pooling operation.\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    # 4. Initialize the output feature map after pooling operation with zeros.\n",
    "    S = np.zeros((n_H, n_W, n_C))\n",
    "    \n",
    "    # 5. Do Pooling Operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Get the slice. (Note that there's only one channel involved. Not like conv_forward)\n",
    "                A_prev_slice = A_prev[vert_head:vert_tail, hori_head:hori_tail, c]\n",
    "                \n",
    "                # Pooling operation\n",
    "                if mode == \"maxpooling\":\n",
    "                    S[h, w, c] = np.max(A_prev_slice)\n",
    "                elif mode == \"avgpooling\":\n",
    "                    S[h, w, c] = np.mean(A_prev_slice)\n",
    "                    \n",
    "    # 6. Check if the output feature map have the valid shape.\n",
    "    assert(S.shape == (n_H, n_W, n_C))\n",
    "    \n",
    "    # 7. Store the cache for backward propagation\n",
    "    cache = (A_prev, hparam)\n",
    "    \n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Conv Backward Propagation #\n",
    "#############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> dC: gradient of the cost with respect to the output of the conv layer (C). The shape is (n_H, n_W, n_C).\n",
    "#  2. <dictionary> cache: Cache of output of conv_forward()\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> dS_prev: gradient of the cost w.r.t. the input of the conv layer (S). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <ndarray> dK: gradient of the cost w.r.t. the weights of the conv layer (K). The shape is (f, f, n_C_prev, n_C).\n",
    "#  3. <ndarray> db: gradient of the cost w.r.t. the biases of the conv layer (b). The shape is (1, 1, 1, n_C).\n",
    "\n",
    "def conv_backward(dC, cache):\n",
    "    \n",
    "    # 1. Retrieve info. from cache.\n",
    "    (S_prev, K, b, hparam) = cache\n",
    "    \n",
    "    # 2. Retrieve the shape of S_prev.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = S_prev.shape\n",
    "    \n",
    "    # 3. Retrieve the shape of Kernel.\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "    \n",
    "    # 4. Retieve info. from hyper parameters.\n",
    "    stride = hparam[\"stride\"]\n",
    "    pad = hparam[\"pad\"]\n",
    "    \n",
    "    # 5. Retrieve the shape of dC\n",
    "    (n_H, n_W, n_C) = dC.shape\n",
    "    \n",
    "    # 6. Initialize dS_prev, dK, db with the correct shapes.\n",
    "    dS_prev = np.zeros((n_H_prev, n_W_prev, n_C_prev))\n",
    "    dK = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    # 7. Pad dS_prev and S_prev\n",
    "    S_prev_pad = __zero_pad(S_prev, pad)\n",
    "    dS_prev_pad = __zero_pad(dS_prev, pad)\n",
    "    \n",
    "    # 8. Do backward pass operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                    \n",
    "                # Get the slice.\n",
    "                S_prev_slice = S_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :]\n",
    "                \n",
    "                # Update Gradients (dS_prev, dK, db) for the window\n",
    "                dS_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :] += K[:,:,:,c] * dC[h, w, c]\n",
    "                dK[: , :, :, c] += S_prev_slice * dC[h, w, c]\n",
    "                db[: , :, :, c] += dC[h, w, c]\n",
    "                \n",
    "    # 9. Unpad dS_prev_pad\n",
    "    if (pad == 0):\n",
    "        dS_prev = dS_prev_pad\n",
    "    else:\n",
    "        dS_prev[:, :, :] = dS_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    \n",
    "    # 10 Check the validity of the shape\n",
    "    assert (dS_prev.shape == (n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dS_prev, dK, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Max Pool Backward helper #\n",
    "############################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __create_mask_from_window(s):\n",
    "    mask = (s == np.max(s))\n",
    "    return mask\n",
    "\n",
    "############################\n",
    "# Avg Pool Backward helper #\n",
    "############################\n",
    "\n",
    "def __distribute_value(ds, shape):\n",
    "    \n",
    "    # 1. Retrieve dimensions from shape\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # 2. Compute the value to distribute on the matrix\n",
    "    average = ds / (n_H * n_W)\n",
    "    \n",
    "    # 3. Create a matrix where each entry is the avg. value.\n",
    "    a = np.ones(shape) * average\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Pool Backward Propagation #\n",
    "#############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> dS: gradient of cost w.r.t. the output of the pooling layer. The shape is the same as the shape of S.\n",
    "#  2. <dictionary> cache: It contaions the output from the forward pass.\n",
    "#  3. <string> mode: Switch between \"maxpooling\" and \"avgpooling\".\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> dA_prev: gradient of cost w.r.t. the input of the pooling layer. The shape is the same as the shape of A_prev.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pool_backward(dS, cache, mode = \"maxpooling\"):\n",
    "    \n",
    "    # 1. Retrieve info. from cache\n",
    "    (A_prev, hparam) = cache\n",
    "    \n",
    "    # 2. Retrieve hyper parameters\n",
    "    stride = hparam[\"stride\"]\n",
    "    f = hparam[\"f\"]\n",
    "    \n",
    "    # 3. Retrieve the shapes of A_prev and dS\n",
    "    n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    n_H, n_W, n_C = dS.shape\n",
    "    \n",
    "    # 4. Initialize dA_prev with zeros.\n",
    "    dA_prev = np.zeros((n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    # 5. Do Backward Pass Operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Compute the backward propagation in both modes\n",
    "                if mode == \"maxpooling\":\n",
    "                    # Use the corners and the specific \"c\" tp defome the current slice of A_prev\n",
    "                    A_prev_slice = A_prev[vert_head:vert_tail, hori_head:hori_tail, c]\n",
    "                    \n",
    "                    # Create the mask from A_prev_slice\n",
    "                    mask = __create_mask_from_window(A_prev_slice)\n",
    "                    \n",
    "                    # Update dA_prev\n",
    "                    dA_prev[vert_head:vert_tail, hori_head:hori_tail, c] += np.multiply(mask, dS[h, w, c])\n",
    "                elif mode == \"avgpooling\":\n",
    "                    # Get the entry ds from dS\n",
    "                    ds = dS[h, w, c]\n",
    "                    \n",
    "                    # Define the shape of the kernel as (f, f).\n",
    "                    shape = (f, f)\n",
    "                    \n",
    "                    # Distribute it (ds) to the correct slice of dA_prev\n",
    "                    dA_prev[vert_head:vert_tail, hori_head:hori_tail, c] += __distribute_value(ds, shape)\n",
    "    \n",
    "    # 6. Check the dA_prev has the valid shape \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Actication Functions for Propagation #\n",
    "########################################\n",
    "\n",
    "# [Input Vars]\n",
    "#   1. <ndarray> Z\n",
    "#\n",
    "# [Output Vars]\n",
    "#   1. <ndarray> A\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def activation_forward(Z, mode):\n",
    "    if mode == \"sigmoid\":\n",
    "        A = 1/(1 + np.exp(-Z))\n",
    "    elif mode == \"relu\":\n",
    "        A = Z * (Z > 0)\n",
    "    return A\n",
    "\n",
    "def activation_backward(X, mode):\n",
    "    if mode == \"sigmoid\":\n",
    "        D_Z_local = np.multiply(1 - X, X)\n",
    "    elif mode == \"relu\":\n",
    "        D_Z_local = X\n",
    "        D_Z_local[X<=0] = 0\n",
    "        D_Z_local[X>0] = 1\n",
    "    return D_Z_local\n",
    "\n",
    "# [Input Vars]\n",
    "#   1. <ndarray> A\n",
    "#\n",
    "# [Output Vars]\n",
    "#   1. <ndarray> Y_pred\n",
    "def __softmax(A):\n",
    "    Y_pred = np.exp(A-np.max(A))/np.sum(np.exp(A-np.max(A)))\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiallize the Kernels, Biases, and hparams\n",
    "\n",
    "def Initialize_Parameters(low, high):\n",
    "    \n",
    "    # C1\n",
    "    K_C1 = np.random.uniform(low=low, high=high, size=(5, 5, 3, 6))\n",
    "    b_C1 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 6))\n",
    "    hparam_C1 = {\"stride\": 1, \"pad\": 2}\n",
    "\n",
    "    # S2\n",
    "    hparam_S2 = {\"f\": 2, \"stride\": 2}\n",
    "\n",
    "    # C3\n",
    "    K_C3 = np.random.uniform(low=low, high=high, size=(5, 5, 6, 16))\n",
    "    b_C3 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 16))\n",
    "    hparam_C3 = {\"stride\":1, \"pad\": 0}\n",
    "\n",
    "    # S4\n",
    "    hparam_S4 = {\"f\": 2, \"stride\": 2}\n",
    "\n",
    "    # C5\n",
    "    K_C5 = np.random.uniform(low=low, high=high, size=(5, 5, 16, 120))\n",
    "    b_C5 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 120))\n",
    "    hparam_C5 = {\"stride\":1, \"pad\": 0}\n",
    "\n",
    "    # W7\n",
    "    W7 = np.random.uniform(low=low, high=high, size=(120, 84))\n",
    "\n",
    "    # W8\n",
    "    W8 = np.random.uniform(low=low, high=high, size=(84, 50))\n",
    "    \n",
    "    return K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# LeNet5 - Forward Propagation #\n",
    "################################\n",
    "\n",
    "def LeNet5_forward(X, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, pool_mode = \"avgpooling\", act_mode = \"sigmoid\"):\n",
    "    \n",
    "    X_C1, cache_C1 = conv_forward(X, K_C1, b_C1, hparam_C1)\n",
    "    X_A1 = activation_forward(X_C1, act_mode)\n",
    "    X_S2, cache_S2 = pool_forward(X_A1, hparam_S2, pool_mode)\n",
    "    X_C3, cache_C3 = conv_forward(X_S2, K_C3, b_C3, hparam_C3)\n",
    "    X_A3 = activation_forward(X_C3, act_mode)\n",
    "    X_S4, cache_S4 = pool_forward(X_A3, hparam_S4, pool_mode)\n",
    "    X_C5, cache_C5 = conv_forward(X_S4, K_C5, b_C5, hparam_C5)\n",
    "    X_A5 = activation_forward(X_C5, act_mode)\n",
    "    X_A6 = X_A5.reshape(1, 120)\n",
    "    X_Z7 = np.dot(X_A6, W7)\n",
    "    X_A7 = activation_forward(X_Z7, act_mode)\n",
    "    X_Z8 = np.dot(X_A7, W8)\n",
    "    X_A8 = activation_forward(X_Z8, act_mode)\n",
    "\n",
    "    Y_pred = __softmax(X_A8)\n",
    "    \n",
    "    return cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_pred, Y_truth):\n",
    "    Error = (-1 * Y_truth * np.log(Y_pred)).sum()\n",
    "    return Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# LeNet5 - Backward Propagation #\n",
    "#################################\n",
    "\n",
    "def LeNet5_backward(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred, Y_truth, pool_mode = \"avgpooling\", act_mode = \"sigmoid\"):\n",
    "    \n",
    "    D_A8 = Y_pred - Y_truth\n",
    "    \n",
    "    D_Z8_local = activation_backward(X_A8, act_mode)\n",
    "    D_Z8 = np.multiply(D_Z8_local, D_A8)\n",
    "    \n",
    "    D_W8 = np.outer(X_A7, D_Z8)\n",
    "    D_A7 = np.dot(D_Z8, D_W8.T)\n",
    "    \n",
    "    D_Z7_local = activation_backward(X_A7, act_mode)\n",
    "    D_Z7 = np.multiply(D_Z7_local, D_A7)\n",
    "    \n",
    "    D_W7 = np.outer(X_A6, D_Z7)\n",
    "    D_A6 = np.dot(D_Z7, D_W7.T)\n",
    "    D_A5 = D_A6.reshape(1,1,120)\n",
    "    \n",
    "    D_C5_local = activation_backward(X_A5, act_mode)\n",
    "    D_C5 = np.multiply(D_C5_local, D_A5)\n",
    "    D_S4, D_K_C5, D_b_C5 = conv_backward(D_C5, cache_C5)\n",
    "    D_A3 = pool_backward(D_S4, cache_S4, pool_mode)\n",
    "    \n",
    "    D_C3_local = activation_backward(X_A3, act_mode)\n",
    "    D_C3 = np.multiply(D_C3_local, D_A3)\n",
    "    D_S2, D_K_C3, D_b_C3 = conv_backward(D_C3, cache_C3)\n",
    "    D_A1 = pool_backward(D_S2, cache_S2, pool_mode)\n",
    "    \n",
    "    D_C1_local = activation_backward(X_A1, act_mode)\n",
    "    D_C1 = np.multiply(D_C1_local, D_A1)\n",
    "    D_X, D_K_C1, D_b_C1 = conv_backward(D_C1, cache_C1)\n",
    "    \n",
    "    return D_W8, D_W7, D_K_C5, D_b_C5, D_K_C3, D_b_C3, D_K_C1, D_b_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_trainable_parameters(lr, D_W8, W8, D_W7, W7, D_K_C5, K_C5, D_b_C5, b_C5, D_K_C3, K_C3, D_b_C3, b_C3, D_K_C1, K_C1, D_b_C1, b_C1):\n",
    "    \n",
    "    W8 = W8 - lr * D_W8\n",
    "    W7 = W7 - lr * D_W7\n",
    "    K_C5 = K_C5 - lr * D_K_C5\n",
    "    b_C5 = b_C5 - lr * D_b_C5\n",
    "    K_C3 = K_C3 - lr * D_K_C3\n",
    "    b_C3 = b_C3 - lr * D_b_C3\n",
    "    K_C1 = K_C1 - lr * D_K_C1\n",
    "    b_C1 = b_C1 - lr * D_b_C1\n",
    "    \n",
    "    return W8, W7, K_C5, b_C5, K_C3, b_C3, K_C1, b_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# [Output Vars]\n",
    "#   1. <int> top1_accuracy\n",
    "#   2. <int> top5_accuracy\n",
    "def top_accuracy(Metadata, Name, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, pool_mode, act_mode):\n",
    "    \n",
    "    num_top1_pred = 0\n",
    "    num_top5_pred = 0\n",
    "    len_dataset = len(Metadata)\n",
    "    \n",
    "    tic = time.time()\n",
    "    for i in range(len_dataset):\n",
    "        # 1. Read a specific image in RGB format.\n",
    "        img = cv.imread(ROOT_PATH + Metadata[i][0])\n",
    "        img_label = Metadata[i][1]\n",
    "    \n",
    "        # 2. Resize the image to a fixed size (128, 128)\n",
    "        img_resize = cv.resize(img, (28, 28))\n",
    "        X = img_resize / 255.0\n",
    "        \n",
    "        cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred = LeNet5_forward(X, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, pool_mode, act_mode)\n",
    "        \n",
    "        # 4. Grab top 5 predictions.\n",
    "        top_1, top_2, top_3, top_4, top_5 = grab_top_5_predictions(Y_pred)\n",
    "        \n",
    "        # 5. Check if the label is the top 1 prediction.\n",
    "        if img_label == top_1: num_top1_pred = num_top1_pred + 1\n",
    "\n",
    "        # 6. Check if the label is in the top 5 predictions\n",
    "        if img_label in [top_1, top_2, top_3, top_4, top_5]: num_top5_pred = num_top5_pred + 1\n",
    "         \n",
    "        #print(f\"top5 are {top_1, top_2, top_3, top_4, top_5}, answer is {img_label}\")\n",
    "        \n",
    "    top1_accuracy = round(num_top1_pred/len_dataset*100, 2)\n",
    "    top5_accuracy = round(num_top5_pred/len_dataset*100, 2)\n",
    "    print(f\"[Result of {Name}] The top-1 accuracy is {top1_accuracy} %\")\n",
    "    print(f\"[Result of {Name}] The top-5 accuracy is {top5_accuracy} %\")\n",
    "    \n",
    "    return top1_accuracy, top5_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Input Vars]\n",
    "#   1. <ndarray> Y_pred: It's a 1-D ndarray which contains the possibilities of the predictions.\n",
    "\n",
    "# [Output Vars]\n",
    "#   1. <int> top_1: The 1st likely breed among those 50 breeds.\n",
    "#   2. <int> top_2: The 2nd likely breed among those 50 breeds.\n",
    "#   3. <int> top_3: The 3rd likely breed among those 50 breeds.\n",
    "#   4. <int> top_4: The 4th likely breed among those 50 breeds.\n",
    "#   5. <int> top_5: The 5th likely breed among those 50 breeds.\n",
    "import numpy as np\n",
    "def grab_top_5_predictions(Y_pred):\n",
    "\n",
    "    top_1 = Y_pred.argmax()\n",
    "    Y_pred[0][top_1] = 0\n",
    "\n",
    "    top_2 = Y_pred.argmax()\n",
    "    Y_pred[0][top_2] = 0\n",
    "    \n",
    "    top_3 = Y_pred.argmax()\n",
    "    Y_pred[0][top_3] = 0\n",
    "\n",
    "    top_4 = Y_pred.argmax()\n",
    "    Y_pred[0][top_4] = 0\n",
    "\n",
    "    top_5 = Y_pred.argmax()\n",
    "\n",
    "    return top_1, top_2, top_3, top_4, top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] There are 63324 pairs in train.txt.\n",
      "[Check] There are 449 pairs in test.txt.\n",
      "[Check] There are 449 pairs in val.txt.\n",
      "\n",
      "[Result of Val] The top-1 accuracy is 2.0 %\n",
      "[Result of Val] The top-5 accuracy is 10.02 %\n",
      "[Epoch: 0 || (0/5000)] The val top-1 Acc. is 2.0, val top-5 Acc. is 10.02.\n",
      "\n",
      "[Epoch: 0 || (0/5000)] [Measure Accuracy] Spend 55.87 sec.\n",
      "[Epoch: 0 || (1/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (1/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (2/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (2/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (3/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (3/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (4/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (4/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (5/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (5/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (6/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (6/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (7/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (7/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (8/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (8/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (9/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (9/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (10/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (10/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (11/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (11/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (12/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (12/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (13/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (13/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (14/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (14/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (15/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (15/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (16/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (16/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (17/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (17/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (18/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (18/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (19/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (19/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (20/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (20/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (21/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (21/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (22/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (22/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (23/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (23/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (24/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (24/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (25/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (25/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (26/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (26/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (27/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (27/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (28/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (28/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (29/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (29/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (30/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (30/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (31/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (31/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (32/63324)] [Forward Propagation] Spend 0.16 sec.\n",
      "[Epoch: 0 || (32/63324)] [Backward Propagation] Spend 0.3 sec.\n",
      "[Epoch: 0 || (33/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (33/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (34/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (34/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (35/63324)] [Forward Propagation] Spend 0.09 sec.\n",
      "[Epoch: 0 || (35/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (36/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (36/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (37/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (37/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (38/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (38/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (39/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (39/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (40/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (40/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (41/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (41/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (42/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (42/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (43/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (43/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (44/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (44/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (45/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (45/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (46/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (46/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (47/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (47/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (48/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (48/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (49/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (49/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (50/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (50/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (51/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (51/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (52/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (52/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (53/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (53/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (54/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (54/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (55/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (55/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (56/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (56/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (57/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (57/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (58/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (58/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (59/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (59/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (60/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (60/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (61/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (61/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (62/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (62/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (63/63324)] [Forward Propagation] Spend 0.1 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0 || (63/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (64/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (64/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (65/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (65/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (66/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (66/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (67/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (67/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (68/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (68/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (69/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (69/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (70/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (70/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (71/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (71/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (72/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (72/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (73/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (73/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (74/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (74/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (75/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (75/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (75/63324)] [Loss] 3.9001420587140916\n",
      "[Check 1st D_K]:\n",
      "[[-6.34354815e-07 -6.43826009e-07 -7.31652495e-07 -7.71895763e-07\n",
      "  -7.21794558e-07]\n",
      " [-5.35330811e-07 -5.26639538e-07 -6.08206642e-07 -6.40112538e-07\n",
      "  -6.06378463e-07]\n",
      " [-4.10257924e-07 -4.16761170e-07 -4.94150654e-07 -5.48251000e-07\n",
      "  -5.18924089e-07]\n",
      " [-3.78636842e-07 -3.72819279e-07 -4.47677289e-07 -5.03360067e-07\n",
      "  -4.90020462e-07]\n",
      " [-3.09640937e-07 -3.13987812e-07 -3.88464296e-07 -4.55293911e-07\n",
      "  -4.63341206e-07]]\n",
      "\n",
      "[Epoch: 0 || (76/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (76/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (77/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (77/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (78/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (78/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (79/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (79/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (80/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (80/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (81/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (81/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (82/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (82/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (83/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (83/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (84/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (84/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (85/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (85/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (86/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (86/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (87/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (87/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (88/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (88/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (89/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (89/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (90/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (90/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (91/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (91/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (92/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (92/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (93/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (93/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (94/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (94/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (95/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (95/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (96/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (96/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (97/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (97/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (98/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (98/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (99/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (99/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (100/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (100/63324)] [Backward Propagation] Spend 0.27 sec.\n",
      "[Epoch: 0 || (101/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (101/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (102/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (102/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (103/63324)] [Forward Propagation] Spend 0.15 sec.\n",
      "[Epoch: 0 || (103/63324)] [Backward Propagation] Spend 0.28 sec.\n",
      "[Epoch: 0 || (104/63324)] [Forward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (104/63324)] [Backward Propagation] Spend 0.3 sec.\n",
      "[Epoch: 0 || (105/63324)] [Forward Propagation] Spend 0.17 sec.\n",
      "[Epoch: 0 || (105/63324)] [Backward Propagation] Spend 0.44 sec.\n",
      "[Epoch: 0 || (106/63324)] [Forward Propagation] Spend 0.15 sec.\n",
      "[Epoch: 0 || (106/63324)] [Backward Propagation] Spend 0.31 sec.\n",
      "[Epoch: 0 || (107/63324)] [Forward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (107/63324)] [Backward Propagation] Spend 0.54 sec.\n",
      "[Epoch: 0 || (108/63324)] [Forward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (108/63324)] [Backward Propagation] Spend 0.33 sec.\n",
      "[Epoch: 0 || (109/63324)] [Forward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (109/63324)] [Backward Propagation] Spend 0.47 sec.\n",
      "[Epoch: 0 || (110/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (110/63324)] [Backward Propagation] Spend 0.25 sec.\n",
      "[Epoch: 0 || (111/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (111/63324)] [Backward Propagation] Spend 0.27 sec.\n",
      "[Epoch: 0 || (112/63324)] [Forward Propagation] Spend 0.14 sec.\n",
      "[Epoch: 0 || (112/63324)] [Backward Propagation] Spend 0.25 sec.\n",
      "[Epoch: 0 || (113/63324)] [Forward Propagation] Spend 0.15 sec.\n",
      "[Epoch: 0 || (113/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (114/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (114/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (115/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (115/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (116/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (116/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (117/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (117/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (118/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (118/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (119/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (119/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (120/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (120/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (121/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (121/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (122/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (122/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (123/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (123/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (124/63324)] [Forward Propagation] Spend 0.1 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0 || (124/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (125/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (125/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (126/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (126/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (127/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (127/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (128/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (128/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (129/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (129/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (130/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (130/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (131/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (131/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (132/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (132/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (133/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (133/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (134/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (134/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (135/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (135/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (136/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (136/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (137/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (137/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (138/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (138/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (139/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (139/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (140/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (140/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (141/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (141/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (142/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (142/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (143/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (143/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (144/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (144/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (145/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (145/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (146/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (146/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (147/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (147/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (148/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (148/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (149/63324)] [Forward Propagation] Spend 0.09 sec.\n",
      "[Epoch: 0 || (149/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (150/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (150/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (151/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (151/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (152/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (152/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (153/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (153/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (154/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (154/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (155/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (155/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (156/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (156/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (157/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (157/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (158/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (158/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (159/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (159/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (160/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (160/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (161/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (161/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (162/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (162/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (163/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (163/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (164/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (164/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (165/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (165/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (166/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (166/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (167/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (167/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (168/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (168/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (169/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (169/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (170/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (170/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (171/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (171/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (172/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (172/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (173/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (173/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (174/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (174/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (175/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (175/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (176/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (176/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (177/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (177/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (178/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (178/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (179/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (179/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (180/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (180/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (181/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (181/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (182/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (182/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (183/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (183/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (184/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (184/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (185/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (185/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (186/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (186/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (187/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (187/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (188/63324)] [Forward Propagation] Spend 0.1 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0 || (188/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (189/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (189/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (190/63324)] [Forward Propagation] Spend 0.14 sec.\n",
      "[Epoch: 0 || (190/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (191/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (191/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (192/63324)] [Forward Propagation] Spend 0.15 sec.\n",
      "[Epoch: 0 || (192/63324)] [Backward Propagation] Spend 0.28 sec.\n",
      "[Epoch: 0 || (193/63324)] [Forward Propagation] Spend 0.14 sec.\n",
      "[Epoch: 0 || (193/63324)] [Backward Propagation] Spend 0.29 sec.\n",
      "[Epoch: 0 || (194/63324)] [Forward Propagation] Spend 0.15 sec.\n",
      "[Epoch: 0 || (194/63324)] [Backward Propagation] Spend 0.27 sec.\n",
      "[Epoch: 0 || (195/63324)] [Forward Propagation] Spend 0.15 sec.\n",
      "[Epoch: 0 || (195/63324)] [Backward Propagation] Spend 0.26 sec.\n",
      "[Epoch: 0 || (196/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (196/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (197/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (197/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (198/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (198/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (199/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (199/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (200/63324)] [Forward Propagation] Spend 0.15 sec.\n",
      "[Epoch: 0 || (200/63324)] [Backward Propagation] Spend 0.29 sec.\n",
      "[Epoch: 0 || (201/63324)] [Forward Propagation] Spend 0.16 sec.\n",
      "[Epoch: 0 || (201/63324)] [Backward Propagation] Spend 0.29 sec.\n",
      "[Epoch: 0 || (202/63324)] [Forward Propagation] Spend 0.14 sec.\n",
      "[Epoch: 0 || (202/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (203/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (203/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (204/63324)] [Forward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (204/63324)] [Backward Propagation] Spend 0.3 sec.\n",
      "[Epoch: 0 || (205/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (205/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (206/63324)] [Forward Propagation] Spend 0.14 sec.\n",
      "[Epoch: 0 || (206/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (207/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (207/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (208/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (208/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (209/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (209/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (210/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (210/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (211/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (211/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (212/63324)] [Forward Propagation] Spend 0.09 sec.\n",
      "[Epoch: 0 || (212/63324)] [Backward Propagation] Spend 0.31 sec.\n",
      "[Epoch: 0 || (213/63324)] [Forward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (213/63324)] [Backward Propagation] Spend 0.34 sec.\n",
      "[Epoch: 0 || (214/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (214/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (215/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (215/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (216/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (216/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (217/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (217/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (218/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (218/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (219/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (219/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (220/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (220/63324)] [Backward Propagation] Spend 0.26 sec.\n",
      "[Epoch: 0 || (221/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (221/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (222/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (222/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (223/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (223/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (224/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (224/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (225/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (225/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (226/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (226/63324)] [Backward Propagation] Spend 0.25 sec.\n",
      "[Epoch: 0 || (227/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (227/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (228/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (228/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (229/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (229/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (230/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (230/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (231/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (231/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (232/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (232/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (233/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (233/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (234/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (234/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (235/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (235/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (236/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (236/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (237/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (237/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (238/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (238/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (239/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (239/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (240/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (240/63324)] [Backward Propagation] Spend 0.19 sec.\n",
      "[Epoch: 0 || (241/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (241/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (242/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (242/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (243/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (243/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (244/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (244/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (245/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (245/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (246/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (246/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (247/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (247/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (248/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (248/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (249/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (249/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (250/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (250/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (251/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (251/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (252/63324)] [Forward Propagation] Spend 0.12 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0 || (252/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (253/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (253/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (254/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (254/63324)] [Backward Propagation] Spend 0.31 sec.\n",
      "[Epoch: 0 || (255/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (255/63324)] [Backward Propagation] Spend 0.25 sec.\n",
      "[Epoch: 0 || (256/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (256/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (257/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (257/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (258/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (258/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (259/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (259/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (260/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (260/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (261/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (261/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (262/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (262/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (263/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (263/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (264/63324)] [Forward Propagation] Spend 0.1 sec.\n",
      "[Epoch: 0 || (264/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (265/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (265/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (266/63324)] [Forward Propagation] Spend 0.11 sec.\n",
      "[Epoch: 0 || (266/63324)] [Backward Propagation] Spend 0.18 sec.\n",
      "[Epoch: 0 || (267/63324)] [Forward Propagation] Spend 0.14 sec.\n",
      "[Epoch: 0 || (267/63324)] [Backward Propagation] Spend 0.24 sec.\n",
      "[Epoch: 0 || (268/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (268/63324)] [Backward Propagation] Spend 0.21 sec.\n",
      "[Epoch: 0 || (269/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (269/63324)] [Backward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (270/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (270/63324)] [Backward Propagation] Spend 0.23 sec.\n",
      "[Epoch: 0 || (271/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (271/63324)] [Backward Propagation] Spend 0.25 sec.\n",
      "[Epoch: 0 || (272/63324)] [Forward Propagation] Spend 0.12 sec.\n",
      "[Epoch: 0 || (272/63324)] [Backward Propagation] Spend 0.22 sec.\n",
      "[Epoch: 0 || (273/63324)] [Forward Propagation] Spend 0.13 sec.\n",
      "[Epoch: 0 || (273/63324)] [Backward Propagation] Spend 0.25 sec.\n",
      "[Epoch: 0 || (274/63324)] [Forward Propagation] Spend 0.16 sec.\n",
      "[Epoch: 0 || (274/63324)] [Backward Propagation] Spend 0.29 sec.\n",
      "[Epoch: 0 || (275/63324)] [Forward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (275/63324)] [Backward Propagation] Spend 0.29 sec.\n",
      "[Epoch: 0 || (276/63324)] [Forward Propagation] Spend 0.2 sec.\n",
      "[Epoch: 0 || (276/63324)] [Backward Propagation] Spend 0.44 sec.\n",
      "[Epoch: 0 || (277/63324)] [Forward Propagation] Spend 0.16 sec.\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = \"C:/Users/USER/Desktop/Projects/Github_Repo/AI/DeepLearning/__HW1_DATA/\"\n",
    "NP_TRAIN_TXT, NP_TEST_TXT, NP_VAL_TXT = read_metadata_files(ROOT_PATH)\n",
    "\n",
    "Metadata = NP_TRAIN_TXT\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize Parameters\n",
    "K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8 = Initialize_Parameters(-0.01, 0.01)\n",
    "Epoch = 10\n",
    "lr = 0.001\n",
    "\n",
    "# 0. Shuffle the training dataset.\n",
    "Length_TRAIN = len(Metadata)\n",
    "random_index = np.arange(Length_TRAIN)\n",
    "np.random.shuffle(random_index)\n",
    "\n",
    "val_top1_list = []\n",
    "val_top5_list = []\n",
    "\n",
    "tic = time.time()\n",
    "val_top1, val_top5 = top_accuracy(NP_VAL_TXT, \"Val\", K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, \"avgpooling\", \"sigmoid\")\n",
    "toc = time.time()\n",
    "print(f\"[Epoch: {0} || ({0}/{5000})] The val top-1 Acc. is {val_top1}, val top-5 Acc. is {val_top5}.\\n\")\n",
    "print(f\"[Epoch: {0} || ({0}/{5000})] [Measure Accuracy] Spend {round(toc-tic,2)} sec.\")\n",
    "\n",
    "val_top1_list.append(val_top1)\n",
    "val_top5_list.append(val_top5)\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(Epoch):\n",
    "    tmp_list = []\n",
    "    counter = 0\n",
    "    for i in random_index[:5000]:\n",
    "    \n",
    "        # 1. Read a specific image in RGB format.\n",
    "        img = cv.imread(ROOT_PATH + Metadata[i][0])\n",
    "        #print(img.shape)\n",
    "        img_label = Metadata[i][1]\n",
    "    \n",
    "        # 2. Resize the image to a fixed size (128, 128)\n",
    "        img_resize = cv.resize(img, (28, 28))\n",
    "        X = img_resize / 255.0\n",
    "        Y_truth = np.zeros((1,50))\n",
    "        Y_truth[0][img_label] = 1\n",
    "    \n",
    "        # 3. Forward Pass\n",
    "        tic = time.time()\n",
    "        cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred = LeNet5_forward(X, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, \"avgpooling\", \"sigmoid\")\n",
    "        toc = time.time()\n",
    "        if i%100 == 0: \n",
    "            print(f\"[Epoch: {epoch} || ({counter+1}/{Length_TRAIN})] [Forward Propagation] Spend {round(toc-tic,2)} sec.\")\n",
    "        \n",
    "        # 4. Cross Entropy Loss\n",
    "        tmp_list.append(cross_entropy(Y_pred, Y_truth))\n",
    "        \n",
    "        # 5. Backward Pass\n",
    "        tic = time.time()\n",
    "        D_W8, D_W7, D_K_C5, D_b_C5, D_K_C3, D_b_C3, D_K_C1, D_b_C1 = LeNet5_backward(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred, Y_truth, \"avgpooling\", \"sigmoid\")\n",
    "        toc = time.time()\n",
    "        if i%100 == 0: \n",
    "            print(f\"[Epoch: {epoch} || ({counter+1}/{Length_TRAIN})] [Backward Propagation] Spend {round(toc-tic,2)} sec.\")\n",
    "        \n",
    "        # 6. Update Weights\n",
    "        W8, W7, K_C5, b_C5, K_C3, b_C3, K_C1, b_C1 = update_trainable_parameters(lr, D_W8, W8, D_W7, W7, D_K_C5, K_C5, D_b_C5, b_C5, D_K_C3, K_C3, D_b_C3, b_C3, D_K_C1, K_C1, D_b_C1, b_C1)    \n",
    "        \n",
    "        if i%100 == 0: \n",
    "            print(f\"[Epoch: {epoch} || ({counter+1}/{Length_TRAIN})] [Loss] {cross_entropy(Y_pred, Y_truth)}\\n[Check 1st D_K]:\\n{D_K_C1[:,:,0,0]}\\n\")\n",
    "        counter = counter + 1\n",
    "        \n",
    "    # 7. Measure Accuracy\n",
    "    tic = time.time()\n",
    "    val_top1, val_top5 = top_accuracy(NP_VAL_TXT, \"Val\", K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, \"avgpooling\", \"sigmoid\")\n",
    "    print(f\"[Epoch: {epoch} || ({counter+1}/{500})] The val top-1 Acc. is {val_top1}, val top-5 Acc. is {val_top5}.\\n\")\n",
    "    toc = time.time()\n",
    "    val_top1_list.append(val_top1)\n",
    "    val_top5_list.append(val_top5)\n",
    "    print(f\"[Epoch: {epoch} || ({counter+1}/{Length_TRAIN})] [Measure Accuracy] Spend {round(toc-tic,2)} sec.\")\n",
    "\n",
    "    loss_list.append(np.mean(tmp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
