{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Pick An Example #\n",
    "###################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <int> i: the index of the training dataset\n",
    "#  2. <dataframe> DF_TRAIN: Dataframe-like format\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> X: Image\n",
    "#  2. <ndarray> Y: The label to the image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pick_an_example(i, DF_TRAIN):\n",
    "    X = DF_TRAIN[i:i+1].values[0][1:].reshape(28,28,1)\n",
    "    Y = DF_TRAIN[i:i+1].values[0][0]\n",
    "    return X, Y\n",
    "\n",
    "#i = np.random.randint(len(DF_TRAIN))\n",
    "#X, Y = pick_an_example(i, DF_TRAIN)\n",
    "#print(f\"The Label of the {i}-th example is {Y}\\nThe corresponding data is at below.\")\n",
    "#plt.imshow(X, cmap=plt.get_cmap('gray'))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Zero Padding #\n",
    "################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> X: Unpadded image. The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <int> pad: expected number of pads on each side. The shape is (n_H_prev + 2 * pad, n_W_prev + 2 * pad, n_C_prev).\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> X_pad: Padded image.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __zero_pad(X, pad):\n",
    "    X_pad = np.pad(X, ((pad, pad), (pad, pad),(0,0)), \"constant\", constant_values = 0)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Conv Single Step #\n",
    "####################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> a_slice_prev: slice of previous feature maps. The shape is (f, f, n_C_prev).\n",
    "#  2. <ndarray> K: A single weight matrix (kernel). The shape is (f, f, n_C_prev).\n",
    "#  3. <ndarray> b: A single bias term. The shape is (1, 1, 1).\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <float> Z: a scalar derived from convolution operation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __conv_single_step(s_slice, K, b):\n",
    "    \n",
    "    S = np.multiply(s_slice, K)\n",
    "    Z = np.sum(S)\n",
    "    Z = Z + float(b)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Conv Forward Propagation #\n",
    "############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> S_prev: The previous feature maps (after activation and pooling). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <ndarray> K: Kernels in a layer. The shape is (f, f, n_C_prev, n_C).\n",
    "#  3. <ndarray> b: biases in a layer. THe shape is (1, 1, 1, n_C).\n",
    "#  4. <dictionary> hparam: this contains hyper parameters like \"pad\" and \"stride\".\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> C: This would be the feature map in the next layer (but before activation). The shape is (n_H, n_W, n_C).\n",
    "#  2. <dictionary> cache: Cache the values needed for backward propagation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def conv_forward(S_prev, K, b, hparam):\n",
    "    \n",
    "    # 1. Retrieve shape of A_prev. We need this to compute the shape of the feature map in the next layer.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = S_prev.shape\n",
    "    \n",
    "    # 2. Retrieve shape of K. We also need this (i.e. f) to compute the shape of the feature map in the next layer.\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "    \n",
    "    # 3. Retrieve info. from hyper parameters. We need them to compute the shape of the feature map in the next layer, too.\n",
    "    stride = hparam[\"stride\"]\n",
    "    pad = hparam[\"pad\"]\n",
    "    \n",
    "    # 4. With info from 1. ~ 3., we can compute the dimension for the feature map in the next layer.\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # 5. Initialize feature maps in the next layer with zeros. Note #Kernel is equal to #Channel of the feature map.\n",
    "    C = np.zeros((n_H, n_W, n_C))\n",
    "    \n",
    "    # 6. Pad S_prev\n",
    "    S_prev_pad = __zero_pad(S_prev, pad)\n",
    "    \n",
    "    # 7. Do Cross-Relation Operation. Note the shape of the output feature map would be (n_H, n_W, n_C).\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                \n",
    "                # Define the corners in the S_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Get the slice.\n",
    "                S_prev_slice = S_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :]\n",
    "                \n",
    "                # Feed it into __conv_single_step(a_slice, K, b). Note we use one kernel and one bias term at once.\n",
    "                C[h, w, c] = __conv_single_step(S_prev_slice, K[:,:,:,c], b[:,:,:,c])\n",
    "    \n",
    "    # 8. Check if the output feature map have the valid shape.\n",
    "    assert(C.shape == (n_H, n_W, n_C))\n",
    "    \n",
    "    # 9. Store the cache for backward propagation\n",
    "    cache = (S_prev, K, b, hparam)\n",
    "    \n",
    "    return C, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Pool Forward Propagation #\n",
    "############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> A_prev: The previous feature maps (after activation). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <dictionary> hparam: It contains \"f\" and \"stride\".\n",
    "#  3. <string> mode: Switch between \"maxpooling\" and \"avgpooling\". The shape is (n_H, n_W, n_C). (n_C = n_C_prev)\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> S: The output feature map after pooling operation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pool_forward(A_prev, hparam, mode = \"maxpooling\"):\n",
    "    # 1. Retrieve shape of A_prev.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # 2. Retrieve info from hyper parameter\n",
    "    f = hparam[\"f\"]\n",
    "    stride = hparam[\"stride\"]\n",
    "\n",
    "    # 3. Define the shape of output of pooling operation.\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    # 4. Initialize the output feature map after pooling operation with zeros.\n",
    "    S = np.zeros((n_H, n_W, n_C))\n",
    "    \n",
    "    # 5. Do Pooling Operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Get the slice. (Note that there's only one channel involved. Not like conv_forward)\n",
    "                A_prev_slice = A_prev[vert_head:vert_tail, hori_head:hori_tail, c]\n",
    "                \n",
    "                # Pooling operation\n",
    "                if mode == \"maxpooling\":\n",
    "                    S[h, w, c] = np.max(A_prev_slice)\n",
    "                elif mode == \"avgpooling\":\n",
    "                    S[h, w, c] = np.mean(A_prev_slice)\n",
    "                    \n",
    "    # 6. Check if the output feature map have the valid shape.\n",
    "    assert(S.shape == (n_H, n_W, n_C))\n",
    "    \n",
    "    # 7. Store the cache for backward propagation\n",
    "    cache = (A_prev, hparam)\n",
    "    \n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Conv Backward Propagation #\n",
    "#############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> dC: gradient of the cost with respect to the output of the conv layer (C). The shape is (n_H, n_W, n_C).\n",
    "#  2. <dictionary> cache: Cache of output of conv_forward()\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> dS_prev: gradient of the cost w.r.t. the input of the conv layer (S). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <ndarray> dK: gradient of the cost w.r.t. the weights of the conv layer (K). The shape is (f, f, n_C_prev, n_C).\n",
    "#  3. <ndarray> db: gradient of the cost w.r.t. the biases of the conv layer (b). The shape is (1, 1, 1, n_C).\n",
    "\n",
    "def conv_backward(dC, cache):\n",
    "    \n",
    "    # 1. Retrieve info. from cache.\n",
    "    (S_prev, K, b, hparam) = cache\n",
    "    \n",
    "    # 2. Retrieve the shape of S_prev.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = S_prev.shape\n",
    "    \n",
    "    # 3. Retrieve the shape of Kernel.\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "    \n",
    "    # 4. Retieve info. from hyper parameters.\n",
    "    stride = hparam[\"stride\"]\n",
    "    pad = hparam[\"pad\"]\n",
    "    \n",
    "    # 5. Retrieve the shape of dC\n",
    "    (n_H, n_W, n_C) = dC.shape\n",
    "    \n",
    "    # 6. Initialize dS_prev, dK, db with the correct shapes.\n",
    "    dS_prev = np.zeros((n_H_prev, n_W_prev, n_C_prev))\n",
    "    dK = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    # 7. Pad dS_prev and S_prev\n",
    "    S_prev_pad = __zero_pad(S_prev, pad)\n",
    "    dS_prev_pad = __zero_pad(dS_prev, pad)\n",
    "    \n",
    "    # 8. Do backward pass operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                    \n",
    "                # Get the slice.\n",
    "                S_prev_slice = S_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :]\n",
    "                \n",
    "                # Update Gradients (dS_prev, dK, db) for the window\n",
    "                dS_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :] += K[:,:,:,c] * dC[h, w, c]\n",
    "                dK[: , :, :, c] += S_prev_slice * dC[h, w, c]\n",
    "                db[: , :, :, c] += dC[h, w, c]\n",
    "                \n",
    "    # 9. Unpad dS_prev_pad\n",
    "    if (pad == 0):\n",
    "        dS_prev = dS_prev_pad\n",
    "    else:\n",
    "        dS_prev[:, :, :] = dS_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    \n",
    "    # 10 Check the validity of the shape\n",
    "    assert (dS_prev.shape == (n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dS_prev, dK, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Max Pool Backward helper #\n",
    "############################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __create_mask_from_window(s):\n",
    "    mask = (s == np.max(s))\n",
    "    return mask\n",
    "\n",
    "############################\n",
    "# Avg Pool Backward helper #\n",
    "############################\n",
    "\n",
    "def __distribute_value(ds, shape):\n",
    "    \n",
    "    # 1. Retrieve dimensions from shape\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # 2. Compute the value to distribute on the matrix\n",
    "    average = ds / (n_H * n_W)\n",
    "    \n",
    "    # 3. Create a matrix where each entry is the avg. value.\n",
    "    a = np.ones(shape) * average\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Pool Backward Propagation #\n",
    "#############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> dS: gradient of cost w.r.t. the output of the pooling layer. The shape is the same as the shape of S.\n",
    "#  2. <dictionary> cache: It contaions the output from the forward pass.\n",
    "#  3. <string> mode: Switch between \"maxpooling\" and \"avgpooling\".\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> dA_prev: gradient of cost w.r.t. the input of the pooling layer. The shape is the same as the shape of A_prev.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pool_backward(dS, cache, mode = \"maxpooling\"):\n",
    "    \n",
    "    # 1. Retrieve info. from cache\n",
    "    (A_prev, hparam) = cache\n",
    "    \n",
    "    # 2. Retrieve hyper parameters\n",
    "    stride = hparam[\"stride\"]\n",
    "    f = hparam[\"f\"]\n",
    "    \n",
    "    # 3. Retrieve the shapes of A_prev and dS\n",
    "    n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    n_H, n_W, n_C = dS.shape\n",
    "    \n",
    "    # 4. Initialize dA_prev with zeros.\n",
    "    dA_prev = np.zeros((n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    # 5. Do Backward Pass Operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Compute the backward propagation in both modes\n",
    "                if mode == \"maxpooling\":\n",
    "                    # Use the corners and the specific \"c\" tp defome the current slice of A_prev\n",
    "                    A_prev_slice = A_prev[vert_head:vert_tail, hori_head:hori_tail, c]\n",
    "                    \n",
    "                    # Create the mask from A_prev_slice\n",
    "                    mask = __create_mask_from_window(A_prev_slice)\n",
    "                    \n",
    "                    # Update dA_prev\n",
    "                    dA_prev[vert_head:vert_tail, hori_head:hori_tail, c] += np.multiply(mask, dS[h, w, c])\n",
    "                elif mode == \"avgpooling\":\n",
    "                    # Get the entry ds from dS\n",
    "                    ds = dS[h, w, c]\n",
    "                    \n",
    "                    # Define the shape of the kernel as (f, f).\n",
    "                    shape = (f, f)\n",
    "                    \n",
    "                    # Distribute it (ds) to the correct slice of dA_prev\n",
    "                    dA_prev[vert_head:vert_tail, hori_head:hori_tail, c] += __distribute_value(ds, shape)\n",
    "    \n",
    "    # 6. Check the dA_prev has the valid shape \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Actication Functions for Propagation #\n",
    "########################################\n",
    "\n",
    "# [Input Vars]\n",
    "#   1. <ndarray> Z\n",
    "#\n",
    "# [Output Vars]\n",
    "#   1. <ndarray> A\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def activation_forward(Z, mode):\n",
    "    if mode == \"sigmoid\":\n",
    "        A = 1/(1 + np.exp(-Z))\n",
    "    elif mode == \"relu\":\n",
    "        A = Z * (Z > 0)\n",
    "    return A\n",
    "\n",
    "def activation_backward(X, mode):\n",
    "    if mode == \"sigmoid\":\n",
    "        D_Z_local = np.multiply(1 - X, X)\n",
    "    elif mode == \"relu\":\n",
    "        D_Z_local = X\n",
    "        D_Z_local[X<=0] = 0\n",
    "        D_Z_local[X>0] = 1\n",
    "    return D_Z_local\n",
    "\n",
    "# [Input Vars]\n",
    "#   1. <ndarray> A\n",
    "#\n",
    "# [Output Vars]\n",
    "#   1. <ndarray> Y_pred\n",
    "def __softmax(A):\n",
    "    Y_pred = np.exp(A-np.max(A))/np.sum(np.exp(A-np.max(A)))\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiallize the Kernels, Biases, and hparams\n",
    "\n",
    "def Initialize_Parameters(low, high):\n",
    "    \n",
    "    # C1\n",
    "    K_C1 = np.random.uniform(low=low, high=high, size=(5, 5, 1, 6))\n",
    "    b_C1 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 6))\n",
    "    hparam_C1 = {\"stride\": 1, \"pad\": 2}\n",
    "\n",
    "    # S2\n",
    "    hparam_S2 = {\"f\": 2, \"stride\": 2}\n",
    "\n",
    "    # C3\n",
    "    K_C3 = np.random.uniform(low=low, high=high, size=(5, 5, 6, 16))\n",
    "    b_C3 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 16))\n",
    "    hparam_C3 = {\"stride\":1, \"pad\": 0}\n",
    "\n",
    "    # S4\n",
    "    hparam_S4 = {\"f\": 2, \"stride\": 2}\n",
    "\n",
    "    # C5\n",
    "    K_C5 = np.random.uniform(low=low, high=high, size=(5, 5, 16, 120))\n",
    "    b_C5 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 120))\n",
    "    hparam_C5 = {\"stride\":1, \"pad\": 0}\n",
    "\n",
    "    # W7\n",
    "    W7 = np.random.uniform(low=low, high=high, size=(120, 84))\n",
    "\n",
    "    # W8\n",
    "    W8 = np.random.uniform(low=low, high=high, size=(84, 10))\n",
    "    \n",
    "    return K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LeNet5 - Forward Propagation\n",
    "\n",
    "def LeNet5_forward(X, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, pool_mode = \"avgpooling\", act_mode = \"sigmoid\"):\n",
    "    \n",
    "    X_C1, cache_C1 = conv_forward(X, K_C1, b_C1, hparam_C1)\n",
    "    X_A1 = activation_forward(X_C1, act_mode)\n",
    "    X_S2, cache_S2 = pool_forward(X_A1, hparam_S2, pool_mode)\n",
    "    X_C3, cache_C3 = conv_forward(X_S2, K_C3, b_C3, hparam_C3)\n",
    "    X_A3 = activation_forward(X_C3, act_mode)\n",
    "    X_S4, cache_S4 = pool_forward(X_A3, hparam_S4, pool_mode)\n",
    "    X_C5, cache_C5 = conv_forward(X_S4, K_C5, b_C5, hparam_C5)\n",
    "    X_A5 = activation_forward(X_C5, act_mode)\n",
    "    X_A6 = X_A5.reshape(1, 120)\n",
    "    X_Z7 = np.dot(X_A6, W7)\n",
    "    X_A7 = activation_forward(X_Z7, act_mode)\n",
    "    X_Z8 = np.dot(X_A7, W8)\n",
    "    #X_A8 = activation_forward(X_Z8, act_mode)\n",
    "    #Y_pred = __softmax(X_A8)\n",
    "    Y_pred = __softmax(X_Z8)\n",
    "    \n",
    "    #return cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred\n",
    "    return cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_pred, Y_truth):\n",
    "    Error = (-1 * Y_truth * np.log(Y_pred)).sum()\n",
    "    return Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet5 - Backward Propagation\n",
    "#def LeNet5_backward(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred, Y_truth, pool_mode = \"avgpooling\", act_mode = \"sigmoid\"):\n",
    "def LeNet5_backward(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, Y_pred, Y_truth, pool_mode = \"avgpooling\", act_mode = \"sigmoid\"):\n",
    "    #D_A8 = Y_pred - Y_truth\n",
    "    \n",
    "    #D_Z8_local = activation_backward(X_A8, act_mode)\n",
    "    \n",
    "    #D_Z8 = np.multiply(D_Z8_local, D_A8)\n",
    "    \n",
    "    D_Z8 = Y_pred - Y_truth\n",
    "    \n",
    "    D_W8 = np.outer(X_A7, D_Z8)\n",
    "    \n",
    "    D_A7 = np.dot(D_Z8, D_W8.T)\n",
    "    \n",
    "    D_Z7_local = activation_backward(X_A7, act_mode)\n",
    "    \n",
    "    D_Z7 = np.multiply(D_Z7_local, D_A7)\n",
    "    \n",
    "    D_W7 = np.outer(X_A6, D_Z7)\n",
    "    \n",
    "    D_A6 = np.dot(D_Z7, D_W7.T)\n",
    "    \n",
    "    D_A5 = D_A6.reshape(1,1,120)\n",
    "    \n",
    "    D_C5_local = activation_backward(X_A5, act_mode)\n",
    "    \n",
    "    D_C5 = np.multiply(D_C5_local, D_A5)\n",
    "    \n",
    "    D_S4, D_K_C5, D_b_C5 = conv_backward(D_C5, cache_C5)\n",
    "    \n",
    "    D_A3 = pool_backward(D_S4, cache_S4, pool_mode)\n",
    "    \n",
    "    D_C3_local = activation_backward(X_A3, act_mode)\n",
    "    \n",
    "    D_C3 = np.multiply(D_C3_local, D_A3)\n",
    "    \n",
    "    D_S2, D_K_C3, D_b_C3 = conv_backward(D_C3, cache_C3)\n",
    "    \n",
    "    D_A1 = pool_backward(D_S2, cache_S2, pool_mode)\n",
    "    \n",
    "    D_C1_local = activation_backward(X_A1, act_mode)\n",
    "    \n",
    "    D_C1 = np.multiply(D_C1_local, D_A1)\n",
    "    \n",
    "    D_X, D_K_C1, D_b_C1 = conv_backward(D_C1, cache_C1)\n",
    "    \n",
    "    return D_W8, D_W7, D_K_C5, D_b_C5, D_K_C3, D_b_C3, D_K_C1, D_b_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_trainable_parameters(lr, D_W8, W8, D_W7, W7, D_K_C5, K_C5, D_b_C5, b_C5, D_K_C3, K_C3, D_b_C3, b_C3, D_K_C1, K_C1, D_b_C1, b_C1):\n",
    "    \n",
    "    W8 = W8 - lr * D_W8\n",
    "    W7 = W7 - lr * D_W7\n",
    "    K_C5 = K_C5 - lr * D_K_C5\n",
    "    b_C5 = b_C5 - lr * D_b_C5\n",
    "    K_C3 = K_C3 - lr * D_K_C3\n",
    "    b_C3 = b_C3 - lr * D_b_C3\n",
    "    K_C1 = K_C1 - lr * D_K_C1\n",
    "    b_C1 = b_C1 - lr * D_b_C1\n",
    "    \n",
    "    return W8, W7, K_C5, b_C5, K_C3, b_C3, K_C1, b_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.zeros(10)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 60000/60000 [00:00<00:00, 85828.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "X_train_tmp, Y_train_tmp, X_test_tmp, Y_test_tmp = load()\n",
    "\n",
    "X_train = np.zeros((len(X_train_tmp), 28, 28, 1))\n",
    "Y_train = np.zeros((len(Y_train_tmp), 10))\n",
    "\n",
    "for i in tqdm(range(len(Y_train_tmp))):\n",
    "    x_tmp = X_train_tmp[i].reshape(28,28,1)\n",
    "    x_tmp = x_tmp/255.\n",
    "    X_train[i] = x_tmp\n",
    "    \n",
    "    y_tmp = np.zeros(10)\n",
    "    y_tmp[Y_train_tmp[i]] = 1\n",
    "    \n",
    "    Y_train[i] = y_tmp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 1/1000 [00:00<04:28,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.211726]\n",
      "[[[0.10951145 0.09761682 0.09683009 0.11562094 0.09667235 0.10073064\n",
      "  0.09987825 0.09208982 0.10325115 0.08779851]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                       | 101/1000 [00:26<03:58,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.971929]\n",
      "[[[0.37835248 0.06842633 0.06795576 0.07805153 0.06787971 0.07007531\n",
      "  0.06966194 0.06526085 0.07146158 0.06287451]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▉                                                               | 201/1000 [00:55<05:39,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.488815]\n",
      "[[[0.61335287 0.04261957 0.04235497 0.04787945 0.04231387 0.04352519\n",
      "  0.04330237 0.04084829 0.04428995 0.03951348]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▊                                                       | 301/1000 [01:22<03:11,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.297702]\n",
      "[[[0.74252229 0.02840134 0.0282363  0.03164739 0.02821085 0.02896311\n",
      "  0.02882548 0.02729586 0.02943714 0.02646026]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▋                                               | 401/1000 [01:48<02:29,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.206691]\n",
      "[[[0.81327128 0.02060607 0.0204916  0.02284431 0.02047398 0.02099486\n",
      "  0.02089976 0.01983858 0.02132257 0.019257  ]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████▌                                       | 501/1000 [02:18<02:12,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.155876]\n",
      "[[[0.85566499 0.01593246 0.01584683 0.01760011 0.01583366 0.01622292\n",
      "  0.01615193 0.01535789 0.01646753 0.01492167]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████▍                               | 601/1000 [02:46<01:57,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12414]\n",
      "[[[0.88325654 0.01288956 0.01282206 0.01420047 0.01281168 0.01311837\n",
      "  0.01306248 0.01243629 0.01331091 0.01209164]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████▍                       | 701/1000 [03:15<01:16,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.102683]\n",
      "[[[0.90241307 0.01077635 0.01072108 0.01184705 0.01071259 0.01096354\n",
      "  0.01091783 0.01040507 0.01112097 0.01012243]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████▎               | 801/1000 [03:42<00:51,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.087312]\n",
      "[[[0.9163913  0.00923405 0.00918752 0.01013373 0.00918037 0.00939156\n",
      "  0.00935311 0.00892134 0.00952396 0.00868304]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████▏       | 901/1000 [04:11<00:25,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.075808]\n",
      "[[[0.92699409 0.00806398 0.00802397 0.00883657 0.00801782 0.00819941\n",
      "  0.00816636 0.00779492 0.00831319 0.0075897 ]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:38<00:00,  3.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fa009f27c0>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Import packages needed to read and print our data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Declare PATH_to_Dataset\n",
    "#ROOT_PATH = \"C:/Users/USER/Desktop/Datasets/Kaggle_Digit_Recognizer/\"\n",
    "\n",
    "# List the files in the ROOT_PATH\n",
    "#print(f\"In the Root Path, there are {os.listdir(ROOT_PATH)}\")\n",
    "\n",
    "# Store the file names in the ROOT_PATH\n",
    "#CSV_TEST = ROOT_PATH + \"test.csv\"\n",
    "#CSV_TRAIN= ROOT_PATH + \"train.csv\"\n",
    "\n",
    "# Initialize Parameters\n",
    "K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8 = Initialize_Parameters(-0.02, 0.02)\n",
    "\n",
    "# Read CSV File Through Pandas\n",
    "#DF_TRAIN = pd.read_csv(CSV_TRAIN)\n",
    "#DF_TRAIN_NO_LABLE = DF_TRAIN.drop(columns=[\"label\"], axis=0)\n",
    "Epoch = 1\n",
    "loss_list = []\n",
    "\n",
    "i = 1\n",
    "lr = 0.001\n",
    "for epoch in range(Epoch):\n",
    "    tmp_list = []\n",
    "    tic = time.time()\n",
    "    #for i in tqdm(range(len(X_train))):\n",
    "    for i in tqdm(range(500)):   \n",
    "        # 1. Pick one example\n",
    "        X, Y_truth = X_train[1], Y_train[1]\n",
    "    \n",
    "        # 2. Forward Pass\n",
    "        #cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred = LeNet5_forward(X, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, \"avgpooling\", \"sigmoid\")\n",
    "        cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, Y_pred = LeNet5_forward(X, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8, \"avgpooling\", \"sigmoid\")\n",
    "    \n",
    "        # 3. Cross Entropy Loss\n",
    "        tmp_list.append(cross_entropy(Y_pred, Y_truth))\n",
    "        \n",
    "        # 4. Backward Pass\n",
    "        #D_W8, D_W7, D_K_C5, D_b_C5, D_K_C3, D_b_C3, D_K_C1, D_b_C1 = LeNet5_backward(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred, Y_truth, \"avgpooling\", \"sigmoid\")\n",
    "        D_W8, D_W7, D_K_C5, D_b_C5, D_K_C3, D_b_C3, D_K_C1, D_b_C1 = LeNet5_backward(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, Y_pred, Y_truth, \"avgpooling\", \"sigmoid\")\n",
    "\n",
    "        # 5. Update Weights\n",
    "        W8, W7, K_C5, b_C5, K_C3, b_C3, K_C1, b_C1 = update_trainable_parameters(lr, D_W8, W8, D_W7, W7, D_K_C5, K_C5, D_b_C5, b_C5, D_K_C3, K_C3, D_b_C3, b_C3, D_K_C1, K_C1, D_b_C1, b_C1)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"[{round(cross_entropy(Y_pred, Y_truth),6)}]\")\n",
    "            print(f\"[{Y_pred}]\")\n",
    "        \n",
    "        loss_list.append(np.mean(tmp_list))\n",
    "    \n",
    "    toc = time.time()\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "\n",
    "# Declare PATH_to_Dataset\n",
    "ROOT_PATH = \"C:/Users/USER/Desktop/Datasets/Kaggle_Digit_Recognizer/\"\n",
    "\n",
    "# List the files in the ROOT_PATH\n",
    "print(f\"In the Root Path, there are {os.listdir(ROOT_PATH)}\")\n",
    "\n",
    "# Store the file names in the ROOT_PATH\n",
    "CSV_TEST = ROOT_PATH + \"test.csv\"\n",
    "CSV_TRAIN= ROOT_PATH + \"train.csv\"\n",
    "\n",
    "DF_TRAIN = pd.read_csv(CSV_TRAIN)\n",
    "\n",
    "print(collections.Counter(DF_TRAIN[\"label\"][:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
