{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Pick An Example #\n",
    "###################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <int> i: the index of the training dataset\n",
    "#  2. <dataframe> DF_TRAIN: Dataframe-like format\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> X: Image\n",
    "#  2. <ndarray> Y: The label to the image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pick_an_example(i, DF_TRAIN):\n",
    "    X = DF_TRAIN[i:i+1].values[0][1:].reshape(28,28,1)\n",
    "    Y = DF_TRAIN[i:i+1].values[0][0]\n",
    "    return X, Y\n",
    "\n",
    "#i = np.random.randint(len(DF_TRAIN))\n",
    "#X, Y = pick_an_example(i, DF_TRAIN)\n",
    "#print(f\"The Label of the {i}-th example is {Y}\\nThe corresponding data is at below.\")\n",
    "#plt.imshow(X, cmap=plt.get_cmap('gray'))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Zero Padding #\n",
    "################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> X: Unpadded image. The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <int> pad: expected number of pads on each side. The shape is (n_H_prev + 2 * pad, n_W_prev + 2 * pad, n_C_prev).\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> X_pad: Padded image.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __zero_pad(X, pad):\n",
    "    X_pad = np.pad(X, ((pad, pad), (pad, pad),(0,0)), \"constant\", constant_values = 0)\n",
    "    return X_pad\n",
    "\n",
    "#X_pad = __zero_pad(X, 2)\n",
    "\n",
    "#plt.imshow(X_pad, cmap=plt.get_cmap('gray'))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Conv Single Step #\n",
    "####################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> a_slice_prev: slice of previous feature maps. The shape is (f, f, n_C_prev).\n",
    "#  2. <ndarray> K: A single weight matrix (kernel). The shape is (f, f, n_C_prev).\n",
    "#  3. <ndarray> b: A single bias term. The shape is (1, 1, 1).\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <float> Z: a scalar derived from convolution operation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __conv_single_step(s_slice, K, b):\n",
    "    \n",
    "    S = np.multiply(s_slice, K)\n",
    "    Z = np.sum(S)\n",
    "    Z = Z + float(b)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Conv Forward Propagation #\n",
    "############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> S_prev: The previous feature maps (after activation and pooling). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <ndarray> K: Kernels in a layer. The shape is (f, f, n_C_prev, n_C).\n",
    "#  3. <ndarray> b: biases in a layer. THe shape is (1, 1, 1, n_C).\n",
    "#  4. <dictionary> hparam: this contains hyper parameters like \"pad\" and \"stride\".\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> C: This would be the feature map in the next layer (but before activation). The shape is (n_H, n_W, n_C).\n",
    "#  2. <dictionary> cache: Cache the values needed for backward propagation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def conv_forward(S_prev, K, b, hparam):\n",
    "    \n",
    "    # 1. Retrieve shape of A_prev. We need this to compute the shape of the feature map in the next layer.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = S_prev.shape\n",
    "    \n",
    "    # 2. Retrieve shape of K. We also need this (i.e. f) to compute the shape of the feature map in the next layer.\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "    \n",
    "    # 3. Retrieve info. from hyper parameters. We need them to compute the shape of the feature map in the next layer, too.\n",
    "    stride = hparam[\"stride\"]\n",
    "    pad = hparam[\"pad\"]\n",
    "    \n",
    "    # 4. With info from 1. ~ 3., we can compute the dimension for the feature map in the next layer.\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # 5. Initialize feature maps in the next layer with zeros. Note #Kernel is equal to #Channel of the feature map.\n",
    "    C = np.zeros((n_H, n_W, n_C))\n",
    "    \n",
    "    # 6. Pad S_prev\n",
    "    S_prev_pad = __zero_pad(S_prev, pad)\n",
    "    \n",
    "    # 7. Do Cross-Relation Operation. Note the shape of the output feature map would be (n_H, n_W, n_C).\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                \n",
    "                # Define the corners in the S_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Get the slice.\n",
    "                S_prev_slice = S_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :]\n",
    "                \n",
    "                # Feed it into __conv_single_step(a_slice, K, b). Note we use one kernel and one bias term at once.\n",
    "                C[h, w, c] = __conv_single_step(S_prev_slice, K[:,:,:,c], b[:,:,:,c])\n",
    "    \n",
    "    # 8. Check if the output feature map have the valid shape.\n",
    "    assert(C.shape == (n_H, n_W, n_C))\n",
    "    \n",
    "    # 9. Store the cache for backward propagation\n",
    "    cache = (S_prev, K, b, hparam)\n",
    "    \n",
    "    return C, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Pool Forward Propagation #\n",
    "############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> A_prev: The previous feature maps (after activation). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <dictionary> hparam: It contains \"f\" and \"stride\".\n",
    "#  3. <string> mode: Switch between \"maxpooling\" and \"avgpooling\". The shape is (n_H, n_W, n_C). (n_C = n_C_prev)\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> S: The output feature map after pooling operation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pool_forward(A_prev, hparam, mode = \"maxpooling\"):\n",
    "    # 1. Retrieve shape of A_prev.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # 2. Retrieve info from hyper parameter\n",
    "    f = hparam[\"f\"]\n",
    "    stride = hparam[\"stride\"]\n",
    "\n",
    "    # 3. Define the shape of output of pooling operation.\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    # 4. Initialize the output feature map after pooling operation with zeros.\n",
    "    S = np.zeros((n_H, n_W, n_C))\n",
    "    \n",
    "    # 5. Do Pooling Operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Get the slice. (Note that there's only one channel involved. Not like conv_forward)\n",
    "                A_prev_slice = A_prev[vert_head:vert_tail, hori_head:hori_tail, c]\n",
    "                \n",
    "                # Pooling operation\n",
    "                if mode == \"maxpooling\":\n",
    "                    S[h, w, c] = np.max(A_prev_slice)\n",
    "                elif mode == \"avgpooling\":\n",
    "                    S[h, w, c] = np.mean(A_prev_slice)\n",
    "                    \n",
    "    # 6. Check if the output feature map have the valid shape.\n",
    "    assert(S.shape == (n_H, n_W, n_C))\n",
    "    \n",
    "    # 7. Store the cache for backward propagation\n",
    "    cache = (A_prev, hparam)\n",
    "    \n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Conv Backward Propagation #\n",
    "#############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> dC: gradient of the cost with respect to the output of the conv layer (C). The shape is (n_H, n_W, n_C).\n",
    "#  2. <dictionary> cache: Cache of output of conv_forward()\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> dS_prev: gradient of the cost w.r.t. the input of the conv layer (S). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <ndarray> dK: gradient of the cost w.r.t. the weights of the conv layer (K). The shape is (f, f, n_C_prev, n_C).\n",
    "#  3. <ndarray> db: gradient of the cost w.r.t. the biases of the conv layer (b). The shape is (1, 1, 1, n_C).\n",
    "\n",
    "def conv_backward(dC, cache):\n",
    "    \n",
    "    # 1. Retrieve info. from cache.\n",
    "    (S_prev, K, b, hparam) = cache\n",
    "    \n",
    "    # 2. Retrieve the shape of S_prev.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = S_prev.shape\n",
    "    \n",
    "    # 3. Retrieve the shape of Kernel.\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "    \n",
    "    # 4. Retieve info. from hyper parameters.\n",
    "    stride = hparam[\"stride\"]\n",
    "    pad = hparam[\"pad\"]\n",
    "    \n",
    "    # 5. Retrieve the shape of dC\n",
    "    (n_H, n_W, n_C) = dC.shape\n",
    "    \n",
    "    # 6. Initialize dS_prev, dK, db with the correct shapes.\n",
    "    dS_prev = np.zeros((n_H_prev, n_W_prev, n_C_prev))\n",
    "    dK = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    # 7. Pad dS_prev and S_prev\n",
    "    S_prev_pad = __zero_pad(S_prev, pad)\n",
    "    dS_prev_pad = __zero_pad(dS_prev, pad)\n",
    "    \n",
    "    # 8. Do backward pass operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                    \n",
    "                # Get the slice.\n",
    "                S_prev_slice = S_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :]\n",
    "                \n",
    "                # Update Gradients (dS_prev, dK, db) for the window\n",
    "                dS_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :] += K[:,:,:,c] * dC[h, w, c]\n",
    "                dK[: , :, :, c] += S_prev_slice * dC[h, w, c]\n",
    "                db[: , :, :, c] += dC[h, w, c]\n",
    "                \n",
    "    # 9. Unpad dS_prev_pad\n",
    "    if (pad == 0):\n",
    "        dS_prev = dS_prev_pad\n",
    "    else:\n",
    "        dS_prev[:, :, :] = dS_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    \n",
    "    # 10 Check the validity of the shape\n",
    "    assert (dS_prev.shape == (n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dS_prev, dK, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Max Pool Backward helper #\n",
    "############################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __create_mask_from_window(s):\n",
    "    mask = (s == np.max(s))\n",
    "    return mask\n",
    "\n",
    "############################\n",
    "# Avg Pool Backward helper #\n",
    "############################\n",
    "\n",
    "def __distribute_value(ds, shape):\n",
    "    \n",
    "    # 1. Retrieve dimensions from shape\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # 2. Compute the value to distribute on the matrix\n",
    "    average = ds / (n_H * n_W)\n",
    "    \n",
    "    # 3. Create a matrix where each entry is the avg. value.\n",
    "    a = np.ones(shape) * average\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Pool Backward Propagation #\n",
    "#############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> dS: gradient of cost w.r.t. the output of the pooling layer. The shape is the same as the shape of S.\n",
    "#  2. <dictionary> cache: It contaions the output from the forward pass.\n",
    "#  3. <string> mode: Switch between \"maxpooling\" and \"avgpooling\".\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> dA_prev: gradient of cost w.r.t. the input of the pooling layer. The shape is the same as the shape of A_prev.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pool_backward(dS, cache, mode = \"maxpooling\"):\n",
    "    \n",
    "    # 1. Retrieve info. from cache\n",
    "    (A_prev, hparam) = cache\n",
    "    \n",
    "    # 2. Retrieve hyper parameters\n",
    "    stride = hparam[\"stride\"]\n",
    "    f = hparam[\"f\"]\n",
    "    \n",
    "    # 3. Retrieve the shapes of A_prev and dS\n",
    "    n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    n_H, n_W, n_C = dS.shape\n",
    "    \n",
    "    # 4. Initialize dA_prev with zeros.\n",
    "    dA_prev = np.zeros((n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    # 5. Do Backward Pass Operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Compute the backward propagation in both modes\n",
    "                if mode == \"maxpooling\":\n",
    "                    # Use the corners and the specific \"c\" tp defome the current slice of A_prev\n",
    "                    A_prev_slice = A_prev[vert_head:vert_tail, hori_head:hori_tail, c]\n",
    "                    \n",
    "                    # Create the mask from A_prev_slice\n",
    "                    mask = __create_mask_from_window(A_prev_slice)\n",
    "                    \n",
    "                    # Update dA_prev\n",
    "                    dA_prev[vert_head:vert_tail, hori_head:hori_tail, c] += np.multiply(mask, dS[h, w, c])\n",
    "                elif mode == \"avgpooling\":\n",
    "                    # Get the entry ds from dS\n",
    "                    ds = dS[h, w, c]\n",
    "                    \n",
    "                    # Define the shape of the kernel as (f, f).\n",
    "                    shape = (f, f)\n",
    "                    \n",
    "                    # Distribute it (ds) to the correct slice of dA_prev\n",
    "                    dA_prev[vert_head:vert_tail, hori_head:hori_tail, c] += __distribute_value(ds, shape)\n",
    "    \n",
    "    # 6. Check the dA_prev has the valid shape \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Functions for Forward Propagation #\n",
    "#####################################\n",
    "\n",
    "# [Input Vars]\n",
    "#   1. <ndarray> Z\n",
    "#\n",
    "# [Output Vars]\n",
    "#   1. <ndarray> A\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "# [Input Vars]\n",
    "#   1. <ndarray> A\n",
    "#\n",
    "# [Output Vars]\n",
    "#   1. <ndarray> Y_pred\n",
    "def __softmax(A):\n",
    "    Y_pred = np.exp(A-np.max(A))/np.sum(np.exp(A-np.max(A)))\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiallize the Kernels, Biases, and hparams\n",
    "\n",
    "def Initialize_Parameters(low, high):\n",
    "    \n",
    "    # C1\n",
    "    K_C1 = np.random.uniform(low=low, high=high, size=(5, 5, 1, 6))\n",
    "    b_C1 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 6))\n",
    "    hparam_C1 = {\"stride\": 1, \"pad\": 2}\n",
    "\n",
    "    # S2\n",
    "    hparam_S2 = {\"f\": 2, \"stride\": 2}\n",
    "\n",
    "    # C3\n",
    "    K_C3 = np.random.uniform(low=low, high=high, size=(5, 5, 6, 16))\n",
    "    b_C3 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 16))\n",
    "    hparam_C3 = {\"stride\":1, \"pad\": 0}\n",
    "\n",
    "    # S4\n",
    "    hparam_S4 = {\"f\": 2, \"stride\": 2}\n",
    "\n",
    "    # C5\n",
    "    K_C5 = np.random.uniform(low=low, high=high, size=(5, 5, 16, 120))\n",
    "    b_C5 = np.random.uniform(low=low, high=high, size=(1, 1, 1, 120))\n",
    "    hparam_C5 = {\"stride\":1, \"pad\": 0}\n",
    "\n",
    "    # W7\n",
    "    W7 = np.random.uniform(low=low, high=high, size=(120, 84))\n",
    "\n",
    "    # W8\n",
    "    W8 = np.random.uniform(low=low, high=high, size=(84, 10))\n",
    "    \n",
    "    return K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LeNet5 - Forward Propagation\n",
    "\n",
    "def LeNet5_forward(X, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8):\n",
    "\n",
    "    #print(f\"[X : Input] The shape of X is {X.shape}.\")\n",
    "\n",
    "    X_C1, cache_C1 = conv_forward(X, K_C1, b_C1, hparam_C1)\n",
    "    #print(f\"[C1: Padding and Convolution] The shape becomes {X_C1.shape}.\")\n",
    "\n",
    "    X_A1 = __sigmoid(X_C1)\n",
    "    #print(f\"[A1: Activatiion] The shape remain {X_A1.shape}\")\n",
    "\n",
    "    X_S2, cache_S2 = pool_forward(X_A1, hparam_S2, mode = \"avgpooling\")\n",
    "    #print(f\"[S2: Pooling] The shape becomes {X_S2.shape}.\")\n",
    "\n",
    "    X_C3, cache_C3 = conv_forward(X_S2, K_C3, b_C3, hparam_C3)\n",
    "    #print(f\"[C3: Padding and Convolution] The shape becomes {X_C3.shape}.\")\n",
    "\n",
    "    X_A3 = __sigmoid(X_C3)\n",
    "    #print(f\"[A3: Activatiion] The shape remain {X_A3.shape}.\")\n",
    "\n",
    "    X_S4, cache_S4 = pool_forward(X_A3, hparam_S4, mode = \"avgpooling\")\n",
    "    #print(f\"[S4: Pooling] The shape becomes {X_S4.shape}.\")\n",
    "\n",
    "    X_C5, cache_C5 = conv_forward(X_S4, K_C5, b_C5, hparam_C5)\n",
    "    #print(f\"[C5: Perform Padding and Convolution] The shape becomes {X_C5.shape}.\")\n",
    "\n",
    "    X_A5 = __sigmoid(X_C5)\n",
    "    #print(f\"[A5: Activatiion] The shape remain {X_A5.shape}.\")\n",
    "\n",
    "    X_A6 = X_A5.reshape(1, 120)\n",
    "    #print(f\"[A6: Reshape] The shape becomes {X_A6.shape}.\")\n",
    "\n",
    "    X_Z7 = np.dot(X_A6, W7)\n",
    "    #print(f\"[Z7: Linear] The shape becomes {X_Z7.shape}.\")\n",
    "\n",
    "    X_A7 = __sigmoid(X_Z7)\n",
    "    #print(f\"[A7: Activatiion] The shape remain {X_A7.shape}.\")\n",
    "\n",
    "    X_Z8 = np.dot(X_A7, W8)\n",
    "    #print(f\"[Z7: Linear] The shape becomes {X_Z8.shape}.\")\n",
    "\n",
    "    X_A8 = __sigmoid(X_Z8)\n",
    "    #print(f\"[A8: Activatiion] The shape remain {X_A8.shape}.\")\n",
    "\n",
    "    Y_pred = __softmax(X_A8)\n",
    "    #print(f\"[Y_pred: Softmax] The shape remain {Y_pred.shape}\")\n",
    "    \n",
    "    return cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_pred, Y_truth):\n",
    "    Error = (-1 * Y_truth * np.log(Y_pred)).sum()\n",
    "    return Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet5 - Backward Propagation\n",
    "def LeNet5_backward(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred, Y_truth):\n",
    "    D_A8 = Y_pred - Y_truth\n",
    "    #print(f\"[D_A8:    Softmax] The shape is {D_A8.shape}.\")\n",
    "\n",
    "    D_Z8_local = np.multiply(1 - X_A8, X_A8)\n",
    "    D_Z8 = np.multiply(D_Z8_local, D_A8)\n",
    "    #print(f\"[D_Z8: Activation] The shape is {D_Z8.shape}.\")\n",
    "\n",
    "    D_W8 = np.outer(X_A7, D_Z8)\n",
    "    #print(f\"[D_W8:    Product] The shape is {D_W8.shape}.\")\n",
    "\n",
    "    D_A7 = np.dot(D_Z8, D_W8.T)\n",
    "    #print(f\"[D_A7:    Product] The shape is {D_A7.shape}.\")\n",
    "\n",
    "    D_Z7_local = np.multiply(1 - X_A7, X_A7)\n",
    "    D_Z7 = np.multiply(D_Z7_local, D_A7)\n",
    "    #print(f\"[D_Z7: Activation] The shape is {D_Z7.shape}.\")\n",
    "\n",
    "    D_W7 = np.outer(X_A6, D_Z7)\n",
    "    #print(f\"[D_W7:    Product] The shape is {D_W7.shape}.\")\n",
    "\n",
    "    D_A6 = np.dot(D_Z7, D_W7.T)\n",
    "    #print(f\"[D_A6:    Product] The shape is {D_A6.shape}.\")\n",
    "\n",
    "    D_A5 = D_A6.reshape(1,1,120)\n",
    "    #print(f\"[D_A5:    Reshape] The shape is {D_A5.shape}.\")\n",
    "\n",
    "    D_C5_local = np.multiply(1 - X_A5, X_A5)\n",
    "    D_C5 = np.multiply(D_C5_local, D_A5)\n",
    "    #print(f\"[D_C5: Activation] The shape is {D_C5.shape}.\")\n",
    "\n",
    "    D_S4, D_K_C5, D_b_C5 = conv_backward(D_C5, cache_C5)\n",
    "    #print(f\"[D_S4: Conv. Back] The shape is {D_S4.shape}.\")\n",
    "    #print(f\"[D_K_C5:   Kernel] The shape is {D_K_C5.shape}.\")\n",
    "    #print(f\"[D_b_C5:     Bias] The shape is {D_b_C5.shape}.\")\n",
    "\n",
    "    D_A3 = pool_backward(D_S4, cache_S4, mode = \"avgpooling\")\n",
    "    #print(f\"[D_A3: Conv. Back] The shape is {D_A3.shape}.\")\n",
    "\n",
    "    D_C3_local = np.multiply(1 - X_A3, X_A3)\n",
    "    D_C3 = np.multiply(D_C3_local, D_A3)\n",
    "    #print(f\"[D_C3: Activation] The shape is {D_C3.shape}.\")\n",
    "\n",
    "    D_S2, D_K_C3, D_b_C3 = conv_backward(D_C3, cache_C3)\n",
    "    #print(f\"[D_S2: Conv. Back] The shape is {D_S2.shape}.\")\n",
    "    #print(f\"[D_K_C3:   Kernel] The shape is {D_K_C3.shape}.\")\n",
    "    #print(f\"[D_b_C3:     Bias] The shape is {D_b_C3.shape}.\")\n",
    "\n",
    "    D_A1 = pool_backward(D_S2, cache_S2, mode = \"avgpooling\")\n",
    "    #print(f\"[D_A1: Conv. Back] The shape is {D_A1.shape}.\")\n",
    "\n",
    "    D_C1_local = np.multiply(1 - X_A1, X_A1)\n",
    "    D_C1 = np.multiply(D_C1_local, D_A1)\n",
    "    #print(f\"[D_C1: Activation] The shape is {D_C1.shape}.\")\n",
    "\n",
    "    D_X, D_K_C1, D_b_C1 = conv_backward(D_C1, cache_C1)\n",
    "    #print(f\"[D_X:  Conv. Back] The shape is {D_X.shape}.\")\n",
    "    #print(f\"[D_K_C1:   Kernel] The shape is {D_K_C1.shape}.\")\n",
    "    #print(f\"[D_b_C1:     Bias] The shape is {D_b_C1.shape}.\")\n",
    "    \n",
    "    return D_W8, D_W7, D_K_C5, D_b_C5, D_K_C3, D_b_C3, D_K_C1, D_b_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_trainable_parameters(lr, D_W8, W8, D_W7, W7, D_K_C5, K_C5, D_b_C5, b_C5, D_K_C3, K_C3, D_b_C3, b_C3, D_K_C1, K_C1, D_b_C1, b_C1):\n",
    "    \n",
    "    W8 = W8 - lr * D_W8\n",
    "    W7 = W7 - lr * D_W7\n",
    "    K_C5 = K_C5 - lr * D_K_C5\n",
    "    b_C5 = b_C5 - lr * D_b_C5\n",
    "    K_C3 = K_C3 - lr * D_K_C3\n",
    "    b_C3 = b_C3 - lr * D_b_C3\n",
    "    K_C1 = K_C1 - lr * D_K_C1\n",
    "    b_C1 = b_C1 - lr * D_b_C1\n",
    "    \n",
    "    return W8, W7, K_C5, b_C5, K_C3, b_C3, K_C1, b_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Root Path, there are ['sample_submission.csv', 'test.csv', 'train.csv']\n",
      "[2.1401] The 0-th example: \n",
      "Y_pred: [[0.0807965  0.11764229 0.1166644  0.07819276 0.07065228 0.09937521\n",
      "  0.14012587 0.08743986 0.10687108 0.10223974]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5163] The 1-th example: \n",
      "Y_pred: [[0.08076047 0.11828622 0.1162813  0.07829826 0.07064136 0.09943991\n",
      "  0.14026093 0.08706418 0.10701417 0.1019532 ]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.132] The 2-th example: \n",
      "Y_pred: [[0.08153364 0.11859868 0.11639306 0.07819351 0.07058171 0.09919785\n",
      "  0.13995746 0.08702546 0.10673342 0.10178522]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.6516] The 3-th example: \n",
      "Y_pred: [[0.08133462 0.11952431 0.11606699 0.07810864 0.07054082 0.09905178\n",
      "  0.13991715 0.08714195 0.10654672 0.10176701]],\n",
      "Y_truth: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5082] The 4-th example: \n",
      "Y_pred: [[0.08141079 0.11915644 0.11577749 0.07821405 0.07096107 0.09921752\n",
      "  0.14014266 0.08690856 0.10658223 0.10162919]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5003] The 5-th example: \n",
      "Y_pred: [[0.08206208 0.11916219 0.11568669 0.07821112 0.07091851 0.09888348\n",
      "  0.13985574 0.0871727  0.10646431 0.10158318]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.442] The 6-th example: \n",
      "Y_pred: [[0.08291119 0.11921207 0.115702   0.07809885 0.07087408 0.09872112\n",
      "  0.13968564 0.08698705 0.10622557 0.10158244]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "[2.5521] The 7-th example: \n",
      "Y_pred: [[0.08276885 0.11903944 0.11543597 0.07791699 0.07088252 0.09871308\n",
      "  0.13957273 0.08801407 0.1061517  0.10150464]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.3175] The 8-th example: \n",
      "Y_pred: [[0.08275068 0.11888193 0.11533185 0.07863936 0.07089915 0.09852307\n",
      "  0.13933746 0.08806515 0.10609703 0.10147432]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5422] The 9-th example: \n",
      "Y_pred: [[0.0826949  0.11897007 0.11493677 0.07869137 0.07093648 0.09972649\n",
      "  0.13926068 0.08791547 0.10578861 0.10107916]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.2474] The 10-th example: \n",
      "Y_pred: [[0.08272823 0.11878997 0.11503793 0.07920069 0.07097672 0.09945572\n",
      "  0.13917369 0.08777095 0.10567832 0.10118778]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "[2.2937] The 11-th example: \n",
      "Y_pred: [[0.08257369 0.11852418 0.11493893 0.07911374 0.07088089 0.0993607\n",
      "  0.13930614 0.08764433 0.10676989 0.1008875 ]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "[2.1343] The 12-th example: \n",
      "Y_pred: [[0.08260313 0.11832754 0.11487911 0.07906072 0.07087757 0.09920973\n",
      "  0.13904573 0.08745669 0.10656232 0.10197746]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5375] The 13-th example: \n",
      "Y_pred: [[0.08246402 0.11911841 0.11471451 0.07906604 0.07079584 0.09898287\n",
      "  0.13882518 0.08740855 0.10657139 0.10205319]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5299] The 14-th example: \n",
      "Y_pred: [[0.08237606 0.11921933 0.11420765 0.07966811 0.07091306 0.09913969\n",
      "  0.1387034  0.08761539 0.10624557 0.10191173]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1281] The 15-th example: \n",
      "Y_pred: [[0.08242471 0.11906167 0.11437564 0.0803054  0.07088985 0.09891659\n",
      "  0.13889012 0.08732925 0.10615109 0.10165569]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1673] The 16-th example: \n",
      "Y_pred: [[0.082468   0.11973053 0.11448624 0.08027626 0.07090917 0.09863701\n",
      "  0.13856183 0.08718034 0.10607857 0.10167205]],\n",
      "Y_truth: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4984] The 17-th example: \n",
      "Y_pred: [[0.08221358 0.11994218 0.11468674 0.08021754 0.07088768 0.09876377\n",
      "  0.13854399 0.08722058 0.10604807 0.10147586]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4397] The 18-th example: \n",
      "Y_pred: [[0.0830401  0.11964361 0.11504544 0.08017197 0.07085738 0.0985318\n",
      "  0.13852053 0.08718325 0.10565918 0.10134674]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "[2.3168] The 19-th example: \n",
      "Y_pred: [[0.08296653 0.11941861 0.11494094 0.08011487 0.07091007 0.09858563\n",
      "  0.13830896 0.08786351 0.10571275 0.10117814]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "[6.68 (s)] \n",
      "In The 0-th epoch: the avg. loss is 2.3797725770822837\n",
      "The 19-th example: \n",
      "Y_pred: [[0.08296653 0.11941861 0.11494094 0.08011487 0.07091007 0.09858563\n",
      "  0.13830896 0.08786351 0.10571275 0.10117814]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1254] The 0-th example: \n",
      "Y_pred: [[0.08288981 0.11938429 0.11475119 0.0799784  0.07091203 0.0994312\n",
      "  0.1381442  0.08809278 0.10524453 0.10117156]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4907] The 1-th example: \n",
      "Y_pred: [[0.08285327 0.11996802 0.11437114 0.08009538 0.07090152 0.09950173\n",
      "  0.13829934 0.0877165  0.10540116 0.10089196]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.118] The 2-th example: \n",
      "Y_pred: [[0.08368433 0.12027759 0.1144812  0.07997855 0.07083983 0.09925918\n",
      "  0.13796496 0.08767347 0.10511595 0.10072494]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.6478] The 3-th example: \n",
      "Y_pred: [[0.08347345 0.12114185 0.11416672 0.07988983 0.07080383 0.09912344\n",
      "  0.13793121 0.08779916 0.1049496  0.10072091]],\n",
      "Y_truth: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4824] The 4-th example: \n",
      "Y_pred: [[0.08354497 0.12078272 0.11387406 0.07999147 0.07122105 0.09929144\n",
      "  0.13815927 0.08756151 0.10498763 0.10058588]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4741] The 5-th example: \n",
      "Y_pred: [[0.08424271 0.12078097 0.11378269 0.07998293 0.07117604 0.09895336\n",
      "  0.13784008 0.08782693 0.10487216 0.10054214]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4346] The 6-th example: \n",
      "Y_pred: [[0.0851481  0.12082755 0.11379373 0.0798558  0.07112829 0.0987904\n",
      "  0.13764762 0.08763587 0.10463207 0.10054058]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "[2.53] The 7-th example: \n",
      "Y_pred: [[0.08499348 0.12066206 0.11353559 0.0796568  0.07114015 0.09878799\n",
      "  0.13752886 0.088653   0.10457017 0.10047191]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.3168] The 8-th example: \n",
      "Y_pred: [[0.08496657 0.12050008 0.1134292  0.08043636 0.07115302 0.09859194\n",
      "  0.13726281 0.08870281 0.10451444 0.10044277]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5196] The 9-th example: \n",
      "Y_pred: [[0.08490342 0.12059683 0.11303817 0.08049093 0.07119233 0.09976532\n",
      "  0.13718591 0.08855295 0.10421797 0.10005616]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.2623] The 10-th example: \n",
      "Y_pred: [[0.08493221 0.12041523 0.11313843 0.08103717 0.07123033 0.09948781\n",
      "  0.13707913 0.08840447 0.10410743 0.10016781]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "[2.3038] The 11-th example: \n",
      "Y_pred: [[0.08476708 0.12015912 0.11304587 0.08094616 0.07113493 0.09940135\n",
      "  0.1372265  0.0882793  0.10516517 0.09987451]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "[2.1205] The 12-th example: \n",
      "Y_pred: [[0.08479775 0.11997532 0.1129975  0.08088958 0.07113599 0.0992606\n",
      "  0.13695568 0.08809436 0.10496994 0.10092328]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5146] The 13-th example: \n",
      "Y_pred: [[0.0846517  0.12070627 0.11284979 0.08089873 0.07105673 0.09904049\n",
      "  0.13673512 0.08805124 0.10499914 0.10101079]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.5066] The 14-th example: \n",
      "Y_pred: [[0.08454898 0.12080304 0.11233721 0.08154299 0.07117364 0.09919911\n",
      "  0.13658667 0.08825958 0.10467712 0.10087167]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1149] The 15-th example: \n",
      "Y_pred: [[0.08459024 0.12064519 0.11249779 0.08222412 0.07114615 0.09897384\n",
      "  0.13676992 0.08796526 0.10457309 0.10061439]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1837] The 16-th example: \n",
      "Y_pred: [[0.08463674 0.12126149 0.11262654 0.0821921  0.07116839 0.09870103\n",
      "  0.13643231 0.08781993 0.10452024 0.10064125]],\n",
      "Y_truth: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4726] The 17-th example: \n",
      "Y_pred: [[0.08436571 0.12146949 0.11281539 0.08213037 0.0711504  0.09883301\n",
      "  0.13641819 0.08786422 0.10449658 0.10045662]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4324] The 18-th example: \n",
      "Y_pred: [[0.08524277 0.12117105 0.1131686  0.08207421 0.07111474 0.09859982\n",
      "  0.1363748  0.08782284 0.10410681 0.10032436]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "[2.3161] The 19-th example: \n",
      "Y_pred: [[0.08516155 0.1209527  0.11307508 0.08200852 0.07116967 0.098657\n",
      "  0.13615328 0.0884862  0.10417291 0.10016309]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "[6.82 (s)] \n",
      "In The 1-th epoch: the avg. loss is 2.368340842067378\n",
      "The 19-th example: \n",
      "Y_pred: [[0.08516155 0.1209527  0.11307508 0.08200852 0.07116967 0.098657\n",
      "  0.13615328 0.0884862  0.10417291 0.10016309]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1126] The 0-th example: \n",
      "Y_pred: [[0.08507627 0.12092709 0.11289798 0.08185884 0.07117547 0.09947084\n",
      "  0.13598076 0.08872459 0.10371739 0.10017077]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4646] The 1-th example: \n",
      "Y_pred: [[0.08503923 0.12145346 0.11252154 0.08198727 0.07116496 0.09954641\n",
      "  0.13615519 0.08834807 0.10388638 0.0998975 ]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1057] The 2-th example: \n",
      "Y_pred: [[0.08592122 0.12176037 0.11263132 0.08185838 0.07110152 0.0993042\n",
      "  0.13579169 0.08830088 0.10359855 0.09973188]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.6441] The 3-th example: \n",
      "Y_pred: [[0.08569886 0.12256642 0.11232894 0.0817655  0.07107004 0.09917827\n",
      "  0.1357634  0.0884355  0.10345188 0.09974118]],\n",
      "Y_truth: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4561] The 4-th example: \n",
      "Y_pred: [[0.08576547 0.12221563 0.11203495 0.08186263 0.07148397 0.09934813\n",
      "  0.13599442 0.08819381 0.10349234 0.09960866]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4476] The 5-th example: \n",
      "Y_pred: [[0.08650243 0.12220679 0.1119451  0.0818486  0.07143674 0.09900682\n",
      "  0.13564462 0.08846059 0.10338077 0.09956754]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4274] The 6-th example: \n",
      "Y_pred: [[0.08745551 0.12225073 0.11195372 0.08170669 0.07138598 0.09884411\n",
      "  0.13543204 0.08826465 0.10314098 0.0995656 ]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "[2.5073] The 7-th example: \n",
      "Y_pred: [[0.08728912 0.12209242 0.11170521 0.08149006 0.07140129 0.09884731\n",
      "  0.13530811 0.08926939 0.10309127 0.09950582]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.3162] The 8-th example: \n",
      "Y_pred: [[0.08725352 0.12192577 0.11159798 0.08232395 0.07141029 0.09864519\n",
      "  0.13501227 0.08931803 0.10303523 0.09947777]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4964] The 9-th example: \n",
      "Y_pred: [[0.08718279 0.1220308  0.11121238 0.08238071 0.07145137 0.09978808\n",
      "  0.13493569 0.08916801 0.10275081 0.09909936]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.2765] The 10-th example: \n",
      "Y_pred: [[0.08720714 0.12184779 0.11131271 0.08296005 0.07148724 0.09950396\n",
      "  0.13481049 0.08901577 0.10264082 0.09921403]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "[2.3134] The 11-th example: \n",
      "Y_pred: [[0.08703205 0.12160194 0.11122761 0.08286535 0.07139234 0.09942664\n",
      "  0.13497355 0.08889244 0.10366021 0.09892788]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "[2.1084] The 12-th example: \n",
      "Y_pred: [[0.08706367 0.12143121 0.1111912  0.08280494 0.0713977  0.09929629\n",
      "  0.13469307 0.08871041 0.10347773 0.09993378]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4911] The 13-th example: \n",
      "Y_pred: [[0.08691124 0.12210563 0.11106013 0.08281795 0.07132041 0.09908226\n",
      "  0.13447239 0.08867209 0.10352584 0.10003206]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4829] The 14-th example: \n",
      "Y_pred: [[0.08679378 0.12219757 0.11054487 0.08349983 0.07143703 0.09924278\n",
      "  0.13429812 0.08888177 0.10320902 0.09989523]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1034] The 15-th example: \n",
      "Y_pred: [[0.08682714 0.12203972 0.11069895 0.08421984 0.07140547 0.09901605\n",
      "  0.13447922 0.08857983 0.10309651 0.09963727]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1996] The 16-th example: \n",
      "Y_pred: [[0.08687681 0.12260709 0.11084483 0.08418476 0.07143013 0.09874945\n",
      "  0.13413292 0.08843798 0.10306238 0.09967363]],\n",
      "Y_truth: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4466] The 17-th example: \n",
      "Y_pred: [[0.08659021 0.12281188 0.11101699 0.08412031 0.07141611 0.09888698\n",
      "  0.13412342 0.08848691 0.10304625 0.09950094]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4254] The 18-th example: \n",
      "Y_pred: [[0.08750933 0.12251417 0.11136553 0.08405405 0.07137534 0.0986537\n",
      "  0.13406232 0.08844179 0.10265768 0.0993661 ]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "[2.3155] The 19-th example: \n",
      "Y_pred: [[0.08742088 0.12230246 0.11128395 0.08397951 0.07143243 0.09871405\n",
      "  0.13383232 0.08908623 0.10273631 0.09921186]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "[6.51 (s)] \n",
      "In The 2-th epoch: the avg. loss is 2.357044657913318\n",
      "The 19-th example: \n",
      "Y_pred: [[0.08742088 0.12230246 0.11128395 0.08397951 0.07143243 0.09871405\n",
      "  0.13383232 0.08908623 0.10273631 0.09921186]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.1014] The 0-th example: \n",
      "Y_pred: [[0.08732706 0.12228554 0.11112028 0.08381634 0.07144188 0.09949603\n",
      "  0.13365266 0.08933335 0.10229362 0.09923325]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4385] The 1-th example: \n",
      "Y_pred: [[0.08728969 0.12275779 0.11074784 0.08395608 0.07143099 0.09957582\n",
      "  0.13384534 0.08895701 0.10247379 0.09896566]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.0951] The 2-th example: \n",
      "Y_pred: [[0.08821321 0.12306263 0.11085883 0.08381552 0.07136621 0.09933493\n",
      "  0.13345563 0.0889059  0.10218515 0.09880199]],\n",
      "Y_truth: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.6403] The 3-th example: \n",
      "Y_pred: [[0.08798028 0.12381386 0.11056874 0.08371837 0.0713388  0.09921821\n",
      "  0.13343181 0.08904908 0.1020572  0.09882364]],\n",
      "Y_truth: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4299] The 4-th example: \n",
      "Y_pred: [[0.08804188 0.12347084 0.11027504 0.08381035 0.07174917 0.09938959\n",
      "  0.13366631 0.08880362 0.10209987 0.09869332]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4213] The 5-th example: \n",
      "Y_pred: [[0.08880884 0.12345564 0.11018872 0.08379124 0.07170004 0.09904605\n",
      "  0.13328869 0.08907193 0.10199358 0.09865526]],\n",
      "Y_truth: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.4206] The 6-th example: \n",
      "Y_pred: [[0.08979887 0.12349793 0.11019679 0.08363505 0.07164669 0.0988846\n",
      "  0.13305911 0.08887177 0.1017557  0.09865348]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "[2.4841] The 7-th example: \n",
      "Y_pred: [[0.08962165 0.12334658 0.10995926 0.08340069 0.07166543 0.09889332\n",
      "  0.13293103 0.08986172 0.10171808 0.09860224]],\n",
      "Y_truth: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[2.3158] The 8-th example: \n",
      "Y_pred: [[0.08957772 0.12317527 0.10985254 0.08428406 0.07167052 0.09868531\n",
      "  0.13260743 0.0899094  0.1016625  0.09857525]],\n",
      "Y_truth: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-4cda5b8c38d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# 4. Backward Pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mD_W8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_W7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_K_C5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_b_C5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_K_C3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_b_C3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_K_C1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_b_C1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeNet5_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_C1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_A1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_S2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_C3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_A3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_S4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_C5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_A5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_A6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_A7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_A8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# 5. Update Weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-e9f3ee2cf054>\u001b[0m in \u001b[0;36mLeNet5_backward\u001b[1;34m(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred, Y_truth)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m#print(f\"[D_C1: Activation] The shape is {D_C1.shape}.\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mD_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_K_C1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_b_C1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_C1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_C1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;31m#print(f\"[D_X:  Conv. Back] The shape is {D_X.shape}.\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m#print(f\"[D_K_C1:   Kernel] The shape is {D_K_C1.shape}.\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-dfe9aa1a1f28>\u001b[0m in \u001b[0;36mconv_backward\u001b[1;34m(dC, cache)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;31m# Update Gradients (dS_prev, dK, db) for the window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mdS_prev_pad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvert_head\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvert_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhori_head\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhori_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[0mdK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mS_prev_slice\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import packages needed to read and print our data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Declare PATH_to_Dataset\n",
    "ROOT_PATH = \"C:/Users/USER/Desktop/Datasets/Kaggle_Digit_Recognizer/\"\n",
    "\n",
    "# List the files in the ROOT_PATH\n",
    "print(f\"In the Root Path, there are {os.listdir(ROOT_PATH)}\")\n",
    "\n",
    "# Store the file names in the ROOT_PATH\n",
    "CSV_TEST = ROOT_PATH + \"test.csv\"\n",
    "CSV_TRAIN= ROOT_PATH + \"train.csv\"\n",
    "\n",
    "# Initialize Parameters\n",
    "K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8 = Initialize_Parameters(-0.3, 0.3)\n",
    "\n",
    "# Read CSV File Through Pandas\n",
    "DF_TRAIN = pd.read_csv(CSV_TRAIN)\n",
    "DF_TRAIN_NO_LABLE = DF_TRAIN.drop(columns=[\"label\"], axis=0)\n",
    "Epoch = 20\n",
    "loss_list = []\n",
    "\n",
    "i = 1\n",
    "lr = 0.01\n",
    "for epoch in range(Epoch):\n",
    "    tmp_list = []\n",
    "    tic = time.time()\n",
    "    for i in range(len(DF_TRAIN[:20])):\n",
    "        \n",
    "        # 1. Pick one example\n",
    "        X, Y = pick_an_example(i, DF_TRAIN)\n",
    "        X = X / 255.0\n",
    "        Y_truth = np.zeros((1,10))\n",
    "        Y_truth[0][Y] = 1\n",
    "    \n",
    "        # 2. Forward Pass\n",
    "        cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred = LeNet5_forward(X, K_C1, b_C1, hparam_C1, hparam_S2, K_C3, b_C3, hparam_C3, hparam_S4, K_C5, b_C5, hparam_C5, W7, W8)\n",
    "    \n",
    "        # 3. Cross Entropy Loss\n",
    "        tmp_list.append(cross_entropy(Y_pred, Y_truth))\n",
    "        \n",
    "        # 4. Backward Pass\n",
    "        D_W8, D_W7, D_K_C5, D_b_C5, D_K_C3, D_b_C3, D_K_C1, D_b_C1 = LeNet5_backward(cache_C1, X_A1, cache_S2, cache_C3, X_A3, cache_S4, cache_C5, X_A5, X_A6, X_A7, X_A8, Y_pred, Y_truth)\n",
    "    \n",
    "        # 5. Update Weights\n",
    "        W8, W7, K_C5, b_C5, K_C3, b_C3, K_C1, b_C1 = update_trainable_parameters(lr, D_W8, W8, D_W7, W7, D_K_C5, K_C5, D_b_C5, b_C5, D_K_C3, K_C3, D_b_C3, b_C3, D_K_C1, K_C1, D_b_C1, b_C1)\n",
    "        #if epoch % 10 == 0: lr = lr * 0.3\n",
    "        \n",
    "        print(f\"[{round(cross_entropy(Y_pred, Y_truth),4)}] The {i}-th example: \\nY_pred: {Y_pred},\\nY_truth: {Y_truth}\\n\")\n",
    "    #loss_list.append(np.mean(tmp_list))\n",
    "    toc = time.time()\n",
    "    #print(f\"[{round(toc - tic, 2)} (s)] \\nIn The {epoch}-th epoch: the avg. loss is {np.mean(tmp_list)}\\nThe {i}-th example: \\nY_pred: {Y_pred},\\nY_truth: {Y_truth}\\n\")\n",
    "    \n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
