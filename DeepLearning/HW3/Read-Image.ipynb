{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages needed to read and print our data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Root Path, there are ['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# Declare PATH_to_Dataset\n",
    "ROOT_PATH = \"C:/Users/USER/Desktop/Datasets/Kaggle_Digit_Recognizer/\"\n",
    "\n",
    "# List the files in the ROOT_PATH\n",
    "print(f\"In the Root Path, there are {os.listdir(ROOT_PATH)}\")\n",
    "\n",
    "# Store the file names in the ROOT_PATH\n",
    "CSV_TEST = ROOT_PATH + \"test.csv\"\n",
    "CSV_TRAIN= ROOT_PATH + \"train.csv\"\n",
    "\n",
    "# Read CSV File Through Pandas\n",
    "DF_TRAIN = pd.read_csv(CSV_TRAIN)\n",
    "DF_TRAIN_NO_LABLE = DF_TRAIN.drop(columns=[\"label\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Label of the 35346-th example is 7\n",
      "The corresponding data is at below.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################\n",
    "# Pick An Example #\n",
    "###################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <int> i: the index of the training dataset\n",
    "#  2. <dataframe> DF_TRAIN: Dataframe-like format\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> X: Image\n",
    "#  2. <ndarray> Y: The label to the image\n",
    "\n",
    "def pick_an_example(i, DF_TRAIN):\n",
    "    X = DF_TRAIN[i:i+1].values[0][1:].reshape(28,28,1)\n",
    "    Y = DF_TRAIN[i:i+1].values[0][0]\n",
    "    return X, Y\n",
    "\n",
    "i = np.random.randint(len(DF_TRAIN))\n",
    "X, Y = pick_an_example(i, DF_TRAIN)\n",
    "print(f\"The Label of the {i}-th example is {Y}\\nThe corresponding data is at below.\")\n",
    "plt.imshow(X, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "################\n",
    "# Zero Padding #\n",
    "################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> X: Unpadded image. The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <int> pad: expected number of pads on each side. The shape is (n_H_prev + 2 * pad, n_W_prev + 2 * pad, n_C_prev).\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> X_pad: Padded image.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __zero_pad(X, pad):\n",
    "    X_pad = np.pad(X, ((pad, pad), (pad, pad),(0,0)), \"constant\", constant_values = 0)\n",
    "    return X_pad\n",
    "\n",
    "X_pad = __zero_pad(X, 2)\n",
    "\n",
    "plt.imshow(X_pad, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Conv Single Step #\n",
    "####################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> a_slice_prev: slice of previous feature maps. The shape is (f, f, n_C_prev).\n",
    "#  2. <ndarray> K: A single weight matrix (kernel). The shape is (f, f, n_C_prev).\n",
    "#  3. <ndarray> b: A single bias term. The shape is (1, 1, 1).\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <float> Z: a scalar derived from convolution operation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __conv_single_step(s_slice, K, b):\n",
    "    \n",
    "    S = np.multiply(s_slice, K)\n",
    "    Z = np.sum(S)\n",
    "    Z = Z + float(b)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Conv Forward Propagation #\n",
    "############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> S_prev: The previous feature maps (after activation and pooling). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <ndarray> K: Kernels in a layer. The shape is (f, f, n_C_prev, n_C).\n",
    "#  3. <ndarray> b: biases in a layer. THe shape is (1, 1, 1, n_C).\n",
    "#  4. <dictionary> hparam: this contains hyper parameters like \"pad\" and \"stride\".\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> C: This would be the feature map in the next layer (but before activation). The shape is (n_H, n_W, n_C).\n",
    "#  2. <dictionary> cache: Cache the values needed for backward propagation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def conv_forward(S_prev, K, b, hparam):\n",
    "    \n",
    "    # 1. Retrieve shape of A_prev. We need this to compute the shape of the feature map in the next layer.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = S_prev.shape\n",
    "    \n",
    "    # 2. Retrieve shape of K. We also need this (i.e. f) to compute the shape of the feature map in the next layer.\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "    \n",
    "    # 3. Retrieve info. from hyper parameters. We need them to compute the shape of the feature map in the next layer, too.\n",
    "    stride = hparam[\"stride\"]\n",
    "    pad = hparam[\"pad\"]\n",
    "    \n",
    "    # 4. With info from 1. ~ 3., we can compute the dimension for the feature map in the next layer.\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # 5. Initialize feature maps in the next layer with zeros. Note #Kernel is equal to #Channel of the feature map.\n",
    "    C = np.zeros((n_H, n_W, n_C))\n",
    "    \n",
    "    # 6. Pad S_prev\n",
    "    S_prev_pad = __zero_pad(S_prev, pad)\n",
    "    \n",
    "    # 7. Do Cross-Relation Operation. Note the shape of the output feature map would be (n_H, n_W, n_C).\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                \n",
    "                # Define the corners in the S_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Get the slice.\n",
    "                S_prev_slice = S_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :]\n",
    "                \n",
    "                # Feed it into __conv_single_step(a_slice, K, b). Note we use one kernel and one bias term at once.\n",
    "                Z[h, w, c] = __conv_single_step(S_prev_slice, K[:,:,:,c], b[:,:,:,c])\n",
    "    \n",
    "    # 8. Check if the output feature map have the valid shape.\n",
    "    assert(C.shape == (n_H, n_W, n_C))\n",
    "    \n",
    "    # 9. Store the cache for backward propagation\n",
    "    cache = (S_prev, K, b, hparam)\n",
    "    \n",
    "    return C, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Pool Forward Propagation #\n",
    "############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> A_prev: The previous feature maps (after activation). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <dictionary> hparam: It contains \"f\" and \"stride\".\n",
    "#  3. <string> mode: Switch between \"maxpooling\" and \"avgpooling\". The shape is (n_H, n_W, n_C). (n_C = n_C_prev)\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> S: The output feature map after pooling operation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pool_forward(A_prev, hparam, mode = \"maxpooling\"):\n",
    "    # 1. Retrieve shape of A_prev.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # 2. Retrieve info from hyper parameter\n",
    "    f = hparam[\"f\"]\n",
    "    stride = hparam[\"stride\"]\n",
    "\n",
    "    # 3. Define the shape of output of pooling operation.\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    # 4. Initialize the output feature map after pooling operation with zeros.\n",
    "    S = np.zeros((n_H, n_W, n_C))\n",
    "    \n",
    "    # 5. Do Pooling Operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Get the slice. (Note that there's only one channel involved. Not like conv_forward)\n",
    "                A_prev_slice = A_prev[vert_head:vert_tail, hori_head:hori_tail, c]\n",
    "                \n",
    "                # Pooling operation\n",
    "                if mode == \"maxpooling\":\n",
    "                    S[h, w, c] = np.max(A_prev_slice)\n",
    "                elif mode == \"avgpooling\":\n",
    "                    S[h, w, c] = np.mean(A_prev_slice)\n",
    "                    \n",
    "    # 6. Check if the output feature map have the valid shape.\n",
    "    assert(S.shape == (n_H, n_W, n_C))\n",
    "    \n",
    "    # 7. Store the cache for backward propagation\n",
    "    cache = (A_prev, hparam)\n",
    "    \n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Conv Backward Propagation #\n",
    "#############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> dC: gradient of the cost with respect to the output of the conv layer (C). The shape is (n_H, n_W, n_C).\n",
    "#  2. <dictionary> cache: Cache of output of conv_forward()\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> dS_prev: gradient of the cost w.r.t. the input of the conv layer (S). The shape is (n_H_prev, n_W_prev, n_C_prev).\n",
    "#  2. <ndarray> dK: gradient of the cost w.r.t. the weights of the conv layer (K). The shape is (f, f, n_C_prev, n_C).\n",
    "#  3. <ndarray> db: gradient of the cost w.r.t. the biases of the conv layer (b). The shape is (1, 1, 1, n_C).\n",
    "\n",
    "def conv_backward(dC, cache):\n",
    "    \n",
    "    # 1. Retrieve info. from cache.\n",
    "    (S_prev, K, b, hparam) = cache\n",
    "    \n",
    "    # 2. Retrieve the shape of S_prev.\n",
    "    (n_H_prev, n_W_prev, n_C_prev) = S_prev.shape\n",
    "    \n",
    "    # 3. Retrieve the shape of Kernel.\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "    \n",
    "    # 4. Retieve info. from hyper parameters.\n",
    "    stride = hparam[\"stride\"]\n",
    "    pad = hparam[\"pad\"]\n",
    "    \n",
    "    # 5. Retrieve the shape of dC\n",
    "    (n_H, n_W, n_C) = dC.shape\n",
    "    \n",
    "    # 6. Initialize dS_prev, dK, db with the correct shapes.\n",
    "    dS_prev = np.zeros((n_H_prev, n_W_prev, n_C_prev))\n",
    "    dK = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    # 7. Pad dS_prev and S_prev\n",
    "    S_prev_pad = __zero_pad(S_prev, pad)\n",
    "    dS_prev_pad = __zero_pad(dS_prev, pad)\n",
    "    \n",
    "    # 8. Do backward pass operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                    \n",
    "                # Get the slice.\n",
    "                S_prev_slice = S_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :]\n",
    "                \n",
    "                # Update Gradients (dS_prev, dK, db) for the window\n",
    "                dS_prev_pad[vert_head:vert_tail, hori_head:hori_tail, :] += K[:,:,:,c] * dC[h, w, c]\n",
    "                dK[: , :, :, c] += S_prev_slice * dC[h, w, c]\n",
    "                db[: , :, :, c] += dC[h, w, c]\n",
    "                \n",
    "    # 9. Unpad dS_prev_pad\n",
    "    dS_prev[:, :, :] = dS_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    # 10 Check the validity of the shape\n",
    "    assert (dS_prev.shape == (n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dS_prev, dK, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Max Pool Backward helper #\n",
    "############################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def __create_mask_from_window(s):\n",
    "    mask = (s == np.max(s))\n",
    "    return mask\n",
    "\n",
    "############################\n",
    "# Avg Pool Backward helper #\n",
    "############################\n",
    "\n",
    "def __distribute_value(ds, shape):\n",
    "    \n",
    "    # 1. Retrieve dimensions from shape\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # 2. Compute the value to distribute on the matrix\n",
    "    average = ds / (n_H * n_W)\n",
    "    \n",
    "    # 3. Create a matrix where each entry is the avg. value.\n",
    "    a = np.ones(shape) * average\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Pool Backward Propagation #\n",
    "#############################\n",
    "\n",
    "#[Input Vars]\n",
    "#  1. <ndarray> dS: gradient of cost w.r.t. the output of the pooling layer. The shape is the same as the shape of S.\n",
    "#  2. <dictionary> cache: It contaions the output from the forward pass.\n",
    "#  3. <string> mode: Switch between \"maxpooling\" and \"avgpooling\".\n",
    "#\n",
    "#[Output Vars]\n",
    "#  1. <ndarray> dA_prev: gradient of cost w.r.t. the input of the pooling layer. The shape is the same as the shape of A_prev.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pool_backward(dS, cache, mode = \"maxpooling\"):\n",
    "    \n",
    "    # 1. Retrieve info. from cache\n",
    "    (A_prev, hparam) = cache\n",
    "    \n",
    "    # 2. Retrieve hyper parameters\n",
    "    stride = hparam[\"stride\"]\n",
    "    f = hparam[\"f\"]\n",
    "    \n",
    "    # 3. Retrieve the shapes of A_prev and dS\n",
    "    n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    n_H, n_W, n_C = dS.shape\n",
    "    \n",
    "    # 4. Initialize dA_prev with zeros.\n",
    "    dA_prev = np.zeros((n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    # 5. Do Backward Pass Operation\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            for c in range(n_C):\n",
    "                                \n",
    "                # Define the corners in the A_prev_pad.\n",
    "                vert_head = h * stride\n",
    "                vert_tail = vert_head + f\n",
    "                hori_head = w * stride\n",
    "                hori_tail = hori_head + f\n",
    "                \n",
    "                # Compute the backward propagation in both modes\n",
    "                if mode == \"maxpooling\":\n",
    "                    # Use the corners and the specific \"c\" tp defome the current slice of A_prev\n",
    "                    A_prev_slice = A_prev[vert_head:vert_tail, hori_head:hori_tail, c]\n",
    "                    \n",
    "                    # Create the mask from A_prev_slice\n",
    "                    mask = __create_mask_from_window(A_prev_slice)\n",
    "                    \n",
    "                    # Update dA_prev\n",
    "                    dA_prev[vert_head:vert_tail, hori_head:hori_tail, c] += np.multiply(mask, dS[h, w, c])\n",
    "                elif mode == \"avgpooling\":\n",
    "                    # Get the entry ds from dS\n",
    "                    ds = dS[h, w, c]\n",
    "                    \n",
    "                    # Define the shape of the kernel as (f, f).\n",
    "                    shape = (f, f)\n",
    "                    \n",
    "                    # Distribute it (ds) to the correct slice of dA_prev\n",
    "                    dA_prev[vert_head:vert_tail, hori_head:hori_tail, c] += __distribute_value(ds, shape)\n",
    "    \n",
    "    # 6. Check the dA_prev has the valid shape \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
