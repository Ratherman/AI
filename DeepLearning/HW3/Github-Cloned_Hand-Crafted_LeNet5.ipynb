{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Load the MNIST Dataset.\n",
    "# [Input Vars] None\n",
    "#\n",
    "# [Pre-requistes]\n",
    "#  1. In the same path should have a pickle file called \"mnist.pkl\".\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> X_train: The shape is 60000 x 784.\n",
    "#  2. <ndarray> Y_train: The shape is 60000 x 1.\n",
    "#  3. <ndarray> X_test: The shape is 10000 x 784.\n",
    "#  4. <ndarray> Y_test: The shape is 10000 x 1.\n",
    "\n",
    "import pickle\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Create log file.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <string> output_path: the path the log file will be saved.\n",
    "#  2. <cfg_name> output_path: the name of the saved log file\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <instance> logger: It's a logging instance.\n",
    "\n",
    "def create_logger(output_path, cfg_name):\n",
    "    log_file = '{}_{}.log'.format(cfg_name, time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    head = '%(asctime)-15s %(message)s'\n",
    "    logging.basicConfig(filename=os.path.join(output_path, log_file), format=head)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Initialize Trainable Parameters.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <dictionary> layers: The layers dictionary stores informations like weights and information in each layer.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  2. <dictionary> new_layers: The layers with initialized parameters.\n",
    "\n",
    "def initialize_parameters(layers):\n",
    "    new_layers = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        mode = layer['mode'] # 'fc', 'conv', 'pool'\n",
    "        if mode == 'pool':\n",
    "            new_layers.append(layer)\n",
    "\n",
    "        elif mode == 'fc':\n",
    "            n_now = layer['n_now']\n",
    "            n_prev = layer['n_prev']\n",
    "            layer['W']=(np.random.rand(n_now, n_prev) - 0.5) * 0.2 # random sample in [-0.1, 0.1] \n",
    "            layer['b']=(np.random.rand(n_now,1) - 0.5) * 0.2\n",
    "            layer['dW']=np.zeros_like(layer['W'])\n",
    "            layer['db']=np.zeros_like(layer['b'])\n",
    "\n",
    "        elif mode == 'conv':\n",
    "            f = layer['f']\n",
    "            n_C = layer['n_C']\n",
    "            n_C_prev = layer['n_C_prev']\n",
    "            layer['W']=(np.random.rand(f, f, n_C_prev, n_C) - 0.5) * 0.2 # random sample in [-0.1, 0.1] \n",
    "            layer['b']=(np.random.rand(1, 1, 1, n_C) - 0.5) * 0.2\n",
    "            layer['dW']=np.zeros_like(layer['W'])\n",
    "            layer['db']=np.zeros_like(layer['b'])\n",
    "\n",
    "        else:\n",
    "            print('Wrong layer in [{}]'.format(i))\n",
    "        \n",
    "        new_layers.append(layer)\n",
    "            \n",
    "    return new_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Define the forward propagation of sigmoid.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> Z: It's the input of actionvation unit.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> A: It's the output of actionvation unit.\n",
    "#  2. <ndarray> cache: We will need this when doing backward propagation.\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "# [Purpose] Define the backward propagation of sigmoid.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> dA: When doing backward propagation. It's the upstream of actionvation unit.\n",
    "#  2. <ndarray> cache: What we stored when doing forward propagation.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> dZ: When doing backward propagation. It's the downstream of actionvation unit.\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Define the forward propagation of relu.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> Z: It's the input of actionvation unit.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> A: It's the output of actionvation unit.\n",
    "#  2. <ndarray> cache: We will need this when doing backward propagation.\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "# [Purpose] Define the backward propagation of relu.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> dA: When doing backward propagation. It's the upstream of actionvation unit.\n",
    "#  2. <ndarray> cache: What we stored when doing forward propagation.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> dZ: When doing backward propagation. It's the downstream of actionvation unit.\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    dZ[Z < 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Define the forwad propagation of softmax.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> Z: The input of the softmax unit.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> A: The output of the softmax unit, which is the prediction.\n",
    "#  2. <ndarray> cache: Maybe it exists just because of consistency purpose, we didn't use it when doing backward propgation.\n",
    "\n",
    "def softmax(Z):\n",
    "    n, m = Z.shape\n",
    "    A = np.exp(Z)\n",
    "    A_sum = np.sum(A, axis = 0)\n",
    "    A_sum = A_sum.reshape(-1, m)\n",
    "    A = A / A_sum\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "# [Purpose] Define the backward propagation of softmax.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> A: Prediction.\n",
    "#  2. <ndarray> Y: Ground Truth.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> dZ: The loss at softmax function, which is at the top the LeNet.\n",
    "\n",
    "def softmax_backward(A, Y):\n",
    "    m = A.shape[1]\n",
    "    dZ = (A - Y) / np.float(m)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Doing forward propagation in fully connected layers.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> A_prev: The output of previous activation function.\n",
    "#  2. <key> layer: Use it to access layer information like bias and weight matrix.\n",
    "#  3. <string> activation: Use it to determine which activation function to use.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> A: The output of the activation unit.\n",
    "#  2. <ndarray> Z: The input of the activation unit. \n",
    "\n",
    "def linear_activation_forward(A_prev, layer, activation='relu'):\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    if activation=='sigmoid':\n",
    "        Z, linear_cache=np.dot(W, A_prev)+b, (A_prev, W, b)\n",
    "        A, activation_cache=sigmoid(Z)\n",
    "    elif activation=='relu':\n",
    "        Z, linear_cache=np.dot(W, A_prev)+b, (A_prev, W, b)\n",
    "        A, activation_cache=relu(Z)\n",
    "    else:\n",
    "        Z = np.dot(W, A_prev)+b\n",
    "        A = Z\n",
    "    return A, Z\n",
    "\n",
    "# [Purpose] Doing Backward propagation in fully connected layers.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> dA: The upstram gradient.\n",
    "#  2. <key> layer: Use it to access layer information like bias and weight matrix.\n",
    "#  3. <string> activation: Use it to determine which activation function to use.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> dA_prev: The downstream gradient.\n",
    "#  2. <ndarray> dW: The corresponding gradient of W: dL/dW.\n",
    "#  3. <ndarray> db: The corresponding gradient of b: dL/db.\n",
    "\n",
    "def linear_activation_backward(dA, layer, activation):\n",
    "    # Backward propagatIon module - linear activation backward\n",
    "    A_prev = layer['A_prev']\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    Z = layer['Z']\n",
    "    if activation=='relu':\n",
    "        dZ=relu_backward(dA, Z)\n",
    "    elif activation=='sigmoid':\n",
    "        dZ=sigmoid_backward(dA, Z)\n",
    "    else:\n",
    "        dZ = dA \n",
    "    n, m = dA.shape\n",
    "    dA_prev=np.dot(W.T, dZ)\n",
    "    dW = np.dot(dZ, A_prev.T)\n",
    "    db = np.sum(dZ, axis = 1).reshape(n,1)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Make the feature map (or input image) with desired dimensio.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> X: Feature maps or input images.\n",
    "#  2. <int> pad: Number of padding.\n",
    "#  3. <value> value: The value of padding pixel, which is usually 0.\n",
    "#\n",
    "# [Output Vars]\n",
    "# 1. <ndarray> X_pad: Padded Feature map (or input image).\n",
    "\n",
    "def zero_pad(X, pad, value = 0):\n",
    "    X_pad = np.pad(X, ((0, 0),(pad, pad),(pad, pad),(0, 0)), 'constant', constant_values=value)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Doing forward propagation in conv. layers.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> A_prev: The output of previous activation function.\n",
    "#  2. <key> layer: Use it to access layer information like bias and weight matrix.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> Z: The feature map after doing conv. operations.\n",
    "\n",
    "def conv_forward(A_prev, layer):\n",
    "\n",
    "    # Retrieve information from layer\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    stride = layer['s']\n",
    "    pad = layer['p']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor\n",
    "    n_H = 1 + int((n_H_prev + 2 * pad - f) / stride)\n",
    "    n_W = 1 + int((n_W_prev + 2 * pad - f) / stride)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    if pad > 0:\n",
    "        A_prev_pad = zero_pad(A_prev, pad)\n",
    "    else:\n",
    "        A_prev_pad = A_prev\n",
    "    \n",
    "    for i in range(m):                                 # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]                     # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (â‰ˆ4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                  \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron\n",
    "                    Z[i, h, w, c] = np.sum(np.multiply(a_slice_prev, W[:, :, :, c])) + b[0, 0, 0, c]\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Purpose] Doing backward propagation in conv. layers.\n",
    "#\n",
    "# [Input Vars]\n",
    "#  1. <ndarray> dZ: The upstream gradient.\n",
    "#  2. <key> layer: Use it to access layer information like bias and weight matrix.\n",
    "#\n",
    "# [Output Vars]\n",
    "#  1. <ndarray> dA_prev: The downstream gradient.\n",
    "#  2. <ndarray> dW: The corresponding gradient of W: dL/dW.\n",
    "#  3. <ndarray> db: The corresponding gradient of b: dL/db.\n",
    "\n",
    "def conv_backward(dZ, layer):\n",
    "\n",
    "    # Retrieve informations from layer\n",
    "    A_prev = layer['A_prev']\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    Z = layer['Z']\n",
    "    stride = layer['s']\n",
    "    pad = layer['p']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    if pad > 0:\n",
    "        A_prev_pad = zero_pad(A_prev, pad)\n",
    "        dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    else:\n",
    "        A_prev_pad = A_prev\n",
    "        dA_prev_pad = np.copy(dA_prev)\n",
    "\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = A_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas\n",
    "                    dA_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += dZ[i, h, w, c] * a_slice\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad\n",
    "        if pad == 0:\n",
    "            dA_prev[i, :, :, :] = dA_prev_pad[i, :, :, :]\n",
    "        else:\n",
    "            dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, layer, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = layer[\"f\"]\n",
    "    stride = layer[\"s\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "\n",
    "    for i in range(m):                           # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c.\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    # Retrieve dimensions from shape\n",
    "    (n_H, n_W) = shape\n",
    "    # Compute the value to distribute on the matrix\n",
    "    average = dz / (n_H * n_W)\n",
    "    # Create a matrix where every entry is the \"average\" value\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a\n",
    "\n",
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, layer, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    # Retrieve information from layer\n",
    "    A_prev = layer['A_prev']\n",
    "    stride = layer['s']\n",
    "    f = layer['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        # select training example from A_prev\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Get the value a from dA\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da.\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(X, layers):\n",
    "    m = X.shape[0]\n",
    "    # -1- convolution layer\n",
    "    layers[0]['A_prev'] = X\n",
    "    Z = conv_forward(X, layers[0])\n",
    "    layers[0]['Z'] = Z\n",
    "    A, _ = relu(Z)\n",
    "    \n",
    "    # -2- average pooling layer\n",
    "    layers[1]['A_prev'] = A\n",
    "    A = pool_forward(A, layers[1], mode = \"average\")\n",
    "    \n",
    "    # -3- convolution layer\n",
    "    layers[2]['A_prev'] = A\n",
    "    Z = conv_forward(A, layers[2])\n",
    "    layers[2]['Z'] = Z\n",
    "    A, _ = relu(Z)\n",
    "    \n",
    "    # -4- average pooling layer\n",
    "    layers[3]['A_prev'] = A\n",
    "    A = pool_forward(A, layers[3], mode = \"average\")\n",
    "    \n",
    "    # -5- convolution layer\n",
    "    layers[4]['A_prev'] = A\n",
    "    Z = conv_forward(A, layers[4])\n",
    "    layers[4]['Z'] = Z\n",
    "    A, _ = relu(Z)\n",
    "    \n",
    "    # -6- fully connected layer\n",
    "    layers[5]['A_prev'] = (A.reshape(m,-1)).T # flatten\n",
    "    A, Z = linear_activation_forward((A.reshape(m,-1)).T, layers[5], activation='relu')\n",
    "    layers[5]['Z'] = Z\n",
    "    \n",
    "    # -7- fully connected layer\n",
    "    layers[6]['A_prev'] = A\n",
    "    _, Z = linear_activation_forward(A, layers[6], activation='none')\n",
    "    layers[6]['Z'] = Z\n",
    "    AL, _ = softmax(Z)\n",
    "\n",
    "    return AL, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    n, m = Y.shape\n",
    "    cost = - np.sum(np.log(AL) * Y) / m\n",
    "    cost=np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propogation(AL, Y, layers):\n",
    "    m = Y.shape[1]\n",
    "    # -7- fully connected layer\n",
    "    dZ = softmax_backward(AL, Y)\n",
    "    dA_prev, dW, db = linear_activation_backward(dZ, layers[6], 'none')\n",
    "    layers[6]['dW'] = dW\n",
    "    layers[6]['db'] = db\n",
    "    \n",
    "    # -6- fully connected layer\n",
    "    dA_prev, dW, db = linear_activation_backward(dA_prev, layers[5], 'relu')\n",
    "    layers[5]['dW'] = dW\n",
    "    layers[5]['db'] = db\n",
    "    \n",
    "    # -5- convolution layer\n",
    "    dA = (dA_prev.T).reshape(m,1,1,layers[4]['n_C']) # flatten backward\n",
    "    dZ = relu_backward(dA, layers[4]['Z'])\n",
    "    dA_prev, dW, db = conv_backward(dZ, layers[4])\n",
    "    layers[4]['dW'] = dW\n",
    "    layers[4]['db'] = db\n",
    "    \n",
    "    # -4- average pooling layer\n",
    "    dA_prev = pool_backward(dA_prev, layers[3], mode = \"average\")\n",
    "    \n",
    "    # -3- convolution layer\n",
    "    dZ = relu_backward(dA_prev, layers[2]['Z'])\n",
    "    dA_prev, dW, db = conv_backward(dZ, layers[2])\n",
    "    layers[2]['dW'] = dW\n",
    "    layers[2]['db'] = db\n",
    "    \n",
    "    # -2- average pooling layer\n",
    "    dA_prev = pool_backward(dA_prev, layers[1], mode = \"average\")\n",
    "    \n",
    "    # -1- convolution layer\n",
    "    dZ = relu_backward(dA_prev, layers[0]['Z'])\n",
    "    dA_prev, dW, db = conv_backward(dZ, layers[0])\n",
    "    layers[0]['dW'] = dW\n",
    "    layers[0]['db'] = db\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(layers, learning_rate):\n",
    "    num_layer = len(layers)\n",
    "    for i in range(num_layer):\n",
    "        mode = layers[i]['mode'] # 'fc', 'conv', 'pool'\n",
    "        if mode == 'pool':\n",
    "            continue\n",
    "        elif (mode == 'fc' or mode == 'conv'):\n",
    "            layers[i]['W'] = layers[i]['W'] - learning_rate*layers[i]['dW']\n",
    "            layers[i]['b'] = layers[i]['b'] - learning_rate*layers[i]['db']\n",
    "        else:\n",
    "            print('Wrong layer mode in [{}]'.format(i))\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, Y_test, layers):\n",
    "    m = X_test.shape[0]\n",
    "    n = Y_test.shape[1]\n",
    "    pred = np.zeros((n,m))\n",
    "    pred_count = np.zeros((n,m)) - 1 # for counting accurate predictions \n",
    "    \n",
    "    # Forward propagation\n",
    "    AL, _ = forward_propogation(X_test, layers)\n",
    "\n",
    "    # convert prediction to 0/1 form\n",
    "    max_index = np.argmax(AL, axis = 0)\n",
    "    pred[max_index, list(range(m))] = 1\n",
    "    pred_count[max_index, list(range(m))] = 1\n",
    "    \n",
    "    accuracy = np.float(np.sum(pred_count == Y_test.T)) / m\n",
    "    \n",
    "    return pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(AL, Y):\n",
    "    n, m = Y.shape\n",
    "    pred_count = np.zeros((n,m)) - 1\n",
    "    \n",
    "    max_index = np.argmax(AL, axis = 0)\n",
    "    pred_count[max_index, list(range(m))] = 1\n",
    "    \n",
    "    accuracy = np.float(np.sum(pred_count == Y)) / m\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mini_batch(X_train, Y_train, X_test, Y_test, layers, logger, num_exp=0, batch_size=10, num_epoch=1, learning_rate=0.01):\n",
    "    logger.info('------------ Integer order CNN with mini batch ------------')\n",
    "    logger.info('Initial weights: FC [-0.1, 0.1], CONV [-0.1, 0.1]')\n",
    "    logger.info('Initial bias: FC [-0.1, 0.1], CONV [-0.1, 0.1]')\n",
    "    logger.info('Batch size: {}'.format(batch_size))\n",
    "    logger.info('Learning rate: {}'.format(learning_rate))\n",
    "    \n",
    "    # number of iteration\n",
    "    num_sample=X_train.shape[0]\n",
    "    num_iteration = num_sample // batch_size\n",
    "    index = list(range(num_sample))\n",
    "    \n",
    "    accuracy_train_list = []\n",
    "    accuracy_test_list = []\n",
    "    for epoch in range(num_epoch):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        random.seed(num_exp*10+epoch)\n",
    "        random.shuffle(index) # random sampling every epoch\n",
    "        for iteration in range(num_iteration):\n",
    "            batch_start = iteration * batch_size\n",
    "            batch_end = (iteration + 1) * batch_size\n",
    "            if batch_end > num_sample:\n",
    "                batch_end = num_sample\n",
    "            X_train_batch = X_train[index[batch_start:batch_end]]\n",
    "            Y_train_batch = Y_train[index[batch_start:batch_end]]\n",
    "            AL, layers = forward_propogation(X_train_batch, layers)\n",
    "            loss = compute_cost(AL, Y_train_batch.T)\n",
    "            accuracy = compute_accuracy(AL, Y_train_batch.T)\n",
    "            layers = backward_propogation(AL, Y_train_batch.T, layers)\n",
    "            layers = update_parameters(layers, learning_rate)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            if (iteration+1) % 600 == 0:\n",
    "                logger.info('Epoch [{}] Iteration [{}]: loss = {} accuracy = {}'.format(epoch, iteration+1, loss, accuracy))\n",
    "                print('Epoch [{}] Iteration [{}]: loss = {} accuracy = {}'.format(epoch, iteration+1, loss, accuracy))\n",
    "                np.save('data/layers_{}_{}.npy'.format(epoch, iteration+1), layers)\n",
    "\n",
    "        _, accuracy_test = predict(X_test, Y_test, layers)\n",
    "        pred_train, _ = forward_propogation(X_train[:10000], layers)\n",
    "        loss_train = compute_cost(pred_train, Y_train[:10000].T)\n",
    "        accuracy_train = compute_accuracy(pred_train, Y_train[:10000].T)\n",
    "        accuracy_train_list.append(accuracy_train)\n",
    "        accuracy_test_list.append(accuracy_test)\n",
    "        print('Epoch [{}] average_loss = {} average_accuracy = {}'.format(epoch, np.mean(losses), np.mean(accuracies)))\n",
    "        logger.info('Epoch [{}] average_loss = {} average_accuracy = {}'.format(epoch, np.mean(losses), np.mean(accuracies)))\n",
    "        print('Epoch [{}] train_loss = {} train_accuracy = {}'.format(epoch, loss_train, accuracy_train))\n",
    "        logger.info('Epoch [{}] train_loss = {} train_accuracy = {}'.format(epoch, loss_train, accuracy_train))\n",
    "        print('Epoch [{}] test_accuracy = {}'.format(epoch, accuracy_test))\n",
    "        logger.info('Epoch [{}] test_accuracy = {}'.format(epoch, accuracy_test))\n",
    "    \n",
    "    return layers, accuracy_train_list, accuracy_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create log file\n",
    "logger = create_logger('output', 'train_log')\n",
    "\n",
    "# Load dataset and reshape image set as (m, n_H, n_W, n_C)\n",
    "#X_train, Y_train = load_mnist('data', 'train')\n",
    "#X_test, Y_test = load_mnist('data', 'test')\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "# Normalization for images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Transform the label into one-hot form\n",
    "(num_train,) = Y_train.shape\n",
    "Y = np.zeros((num_train, 10))\n",
    "for i in range(num_train):\n",
    "    Y[i, Y_train[i]] = 1\n",
    "Y_train = Y\n",
    "(num_test,) = Y_test.shape\n",
    "Y = np.zeros((num_test, 10))\n",
    "for i in range(num_test):\n",
    "    Y[i, Y_test[i]] = 1\n",
    "Y_test = Y\n",
    "\n",
    "# Construct model\n",
    "layer1={}\n",
    "layer1['mode'] = 'conv'\n",
    "layer1['f'] = 5\n",
    "layer1['n_C_prev'] = 1\n",
    "layer1['n_C'] = 6\n",
    "layer1['p'] = 2\n",
    "layer1['s'] = 1\n",
    "layer2={}\n",
    "layer2['mode'] = 'pool'\n",
    "layer2['f'] = 2\n",
    "layer2['s'] = 2\n",
    "layer3={}\n",
    "layer3['mode'] = 'conv'\n",
    "layer3['f'] = 5\n",
    "layer3['n_C_prev'] = 6\n",
    "layer3['n_C'] = 16\n",
    "layer3['p'] = 0\n",
    "layer3['s'] = 1\n",
    "layer4={}\n",
    "layer4['mode'] = 'pool'\n",
    "layer4['f'] = 2\n",
    "layer4['s'] = 2\n",
    "layer5={}\n",
    "layer5['mode'] = 'conv'\n",
    "layer5['f'] = 5\n",
    "layer5['n_C_prev'] = 16\n",
    "layer5['n_C'] = 120\n",
    "layer5['p'] = 0\n",
    "layer5['s'] = 1\n",
    "layer6={}\n",
    "layer6['mode'] = 'fc'\n",
    "layer6['n_now'] = 84\n",
    "layer6['n_prev'] = 120\n",
    "layer7={}\n",
    "layer7['mode'] = 'fc'\n",
    "layer7['n_now'] = 10\n",
    "layer7['n_prev'] = 84\n",
    "construct_layers = [layer1, layer2, layer3, layer4, layer5, layer6, layer7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------- Experiment 1 -------------------------------------\n",
      "Initialize layers and save as data/initial_layers_1.npy\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-36b6e343a22a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----------------------------------------------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----------------------------------------------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     layers, train_acc, test_acc = train_mini_batch(X_train, Y_train, X_test, Y_test, initial_layers,\n\u001b[0m\u001b[0;32m     20\u001b[0m                                     logger, num_exp=index, batch_size=10, num_epoch=1, learning_rate=0.1)\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-34f770e0b8a3>\u001b[0m in \u001b[0;36mtrain_mini_batch\u001b[1;34m(X_train, Y_train, X_test, Y_test, layers, logger, num_exp, batch_size, num_epoch, learning_rate)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_propogation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-250eb723a191>\u001b[0m in \u001b[0;36mbackward_propogation\u001b[1;34m(AL, Y, layers)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# -1- convolution layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Z'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dW'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'db'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-071c952e0873>\u001b[0m in \u001b[0;36mconv_backward\u001b[1;34m(dZ, layer)\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mdA_prev_pad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvert_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvert_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhoriz_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhoriz_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                     \u001b[0mdW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma_slice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                     \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_experiments = 1\n",
    "for index in range(num_experiments):\n",
    "    print('------------------------------------- Experiment {} -------------------------------------'.format(index+1))\n",
    "    logger.info('------------------------------------- Experiment {} -------------------------------------'.format(index+1))\n",
    "\n",
    "    initial_layers_path = 'data/initial_layers_{}.npy'.format(index+1)\n",
    "    if os.path.exists(initial_layers_path):\n",
    "        initial_layers = np.load(initial_layers_path)\n",
    "        print('Load initial parameters from {}'.format(initial_layers_path))\n",
    "        logger.info('Load initial parameters from {}'.format(initial_layers_path))\n",
    "    else:\n",
    "        initial_layers = initialize_parameters(construct_layers)\n",
    "        np.save(initial_layers_path, initial_layers)\n",
    "        print('Initialize layers and save as {}'.format(initial_layers_path))\n",
    "        logger.info('Initialize layers and save as {}'.format(initial_layers_path))\n",
    "    \n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    logger.info('----------------------------------------------------------------------------------------')\n",
    "    layers, train_acc, test_acc = train_mini_batch(X_train, Y_train, X_test, Y_test, initial_layers,\n",
    "                                    logger, num_exp=index, batch_size=10, num_epoch=1, learning_rate=0.1)\n",
    "    print('\\n')\n",
    "    logger.info('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd06178dbce4fd0798ff64fcc205425e6161eb31df92691dd50524911eda6636db7",
   "display_name": "Python 3.8.8 64-bit ('ASE': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}