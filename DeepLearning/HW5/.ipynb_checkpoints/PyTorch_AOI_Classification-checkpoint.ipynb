{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AOI Classification using (A) LeNet5, (B) AlexNet, (C) ResNet\n",
    "\n",
    "## Outline:\n",
    "* Step 01: Get a feel of data, draw it.\n",
    "* Step 02: Define dataset class.\n",
    "--------------------------------\n",
    "* Step A1: Define LeNet5.\n",
    "* Step A2: Setup dataloader for LeNet5.\n",
    "* Step A3: Setup loss and hyper parameter for LeNet5.\n",
    "* Step A4: Train LeNet5.\n",
    "* Step A5: Evaluate the performance of LeNet5.\n",
    "* Step A6: Test LeNet5.\n",
    "--------------------------------\n",
    "* Step B1: Define AlexNet.\n",
    "* Step B2: Setup dataloader for AlexNet5.\n",
    "* Step B3: Setup loss and hyper parameter for AlexNet5.\n",
    "* Step B4: Train AlexNet5.\n",
    "* Step B5: Evaluate the performance of AlexNet5.\n",
    "* Step B6: Test AlexNet5.\n",
    "--------------------------------\n",
    "* Step C1: Define ResNet18.\n",
    "* Step C2: Setup dataloader for ResNet18.\n",
    "* Step C3: Setup loss and hyper parameter for ResNet18.\n",
    "* Step C4: Train ResNet18.\n",
    "* Step C5: Evaluate the performance of ResNet18.\n",
    "* Step C6: Test ResNet18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 01: Get a feel of data, draw it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv # for reading image data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_table = {\n",
    "    0: \"normal\",\n",
    "    1: \"void\",\n",
    "    2: \"Horizontal Defect\",\n",
    "    3: \"Vertical Defect\",\n",
    "    4: \"Edge Defect\",\n",
    "    5: \"Partical\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Label] => 5; [Label Actually Means] => Partical\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "root_train = \"../__HW5_DATA/train_images/\"\n",
    "root_test = \"../__HW5_DATA/test_images/\"\n",
    "train_csv = \"../__HW5_DATA/train.csv\"\n",
    "test_csv = \"../__HW5_DATA/test.csv\"\n",
    "df_train = pd.read_csv(train_csv)\n",
    "\n",
    "id = 4\n",
    "png_img = cv.imread(root_train + df_train.ID[id])\n",
    "label = df_train.Label[id]\n",
    "print(f\"[Label] => {df_train.Label[id]}; [Label Actually Means] => {label_map_table[label]}\")\n",
    "plt.imshow(png_img)\n",
    "plt.show()\n",
    "\n",
    "print(png_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 02: Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "\n",
    "# [Input Args]\n",
    "# 1. target_csv <string>: It's the metadata file describe the name of image and its label.\n",
    "# 2. root_path  <string>: It's the path to the image folder. Combination of this and name is the full path to the image.\n",
    "# 3. height <int>: Use this for elastically resize image to desired shape.\n",
    "# 4. width <int>: Use this for elastically resize image to desired shape.\n",
    "class AOI_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, target_csv, root_path, height, width, transform = None):\n",
    "        \n",
    "        # height, width\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # register self\n",
    "        self.target_csv = target_csv\n",
    "        self.root_path = root_path\n",
    "        \n",
    "        # 1. Read CSV file through root_path\n",
    "        self.df = pd.read_csv(self.target_csv)\n",
    "        \n",
    "        # 2. Remember the length\n",
    "        self.count = len(self.df)\n",
    "        \n",
    "        # 3. transform\n",
    "        self.transfroms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Read images\n",
    "        img = cv.imread(self.root_path + self.df.ID[index])\n",
    "        img_resize = cv.resize(img, (self.height, self.width))\n",
    "\n",
    "        # To Tensor\n",
    "        img_tensor = self.transforms(np.uint8(img_resize))\n",
    "        \n",
    "        # Get label\n",
    "        label = self.df.Label[index]\n",
    "        \n",
    "        return (img_tensor, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step A1: Define LeNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d( 3,  6, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d( 6, 16, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(16, 50, 3, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(50 * 64 * 64, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 50 * 64 * 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "lenet = LeNet()\n",
    "print(lenet.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step A2: Setup dataloader for LeNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((height + 200, width + 200)),\n",
    "    transforms.CenterCrop((height + 100, width + 100)),\n",
    "    transforms.RandomCrop((height, width)),\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    transforms.RandomVerticalFlip(p = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "if torch.cuda.is_available(): summary(lenet, (3, height, width))\n",
    "Train_Dataset = AOI_Dataset(target_csv = train_csv, root_path = root_train, height = height, width = width, transform = transform_train)\n",
    "\n",
    "batch_size = 8\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 43\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(Train_Dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "Train_DataLoader = torch.utils.data.DataLoader(Train_Dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "Val_DataLoader = torch.utils.data.DataLoader(Train_Dataset, batch_size=batch_size, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step A3: Setup loss and hyper parameter for LeNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(lenet.parameters(), lr=1e-4)\n",
    "epoch = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step A4: Train LeNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "loss_list = []\n",
    "print_probe_num = 100\n",
    "for epoch in range(epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(Train_DataLoader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device) # GPU        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = lenet(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % print_probe_num == (print_probe_num - 1):\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / print_probe_num))\n",
    "            loss_list.append(running_loss / print_probe_num)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Train\n",
    "    with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        for datum in tqdm(Train_DataLoader):\n",
    "\n",
    "            imgs, labs = datum[0].to(device), datum[1].to(device)\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = lenet(imgs.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labs.size(0)\n",
    "            correct += (preds == labs).sum().item()\n",
    "        train_acc_list.append(float(correct)/float(total))\n",
    "        print('Accuracy of the network on the train images: %d %%' % (100 * correct / total))\n",
    "  \n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Train\n",
    "    with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        for datum in tqdm(Val_DataLoader):\n",
    "\n",
    "            imgs, labs = datum[0].to(device), datum[1].to(device)\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = lenet(imgs.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labs.size(0)\n",
    "            correct += (preds == labs).sum().item()\n",
    "        val_acc_list.append(float(correct)/float(total))\n",
    "        print('Accuracy of the network on the val images: %d %%' % (100 * correct / total))\n",
    "toc = time.time()\n",
    "print(f\"Spend {round(toc - tic, 2)} (sec)\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step A5: Evaluate the performance of LeNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.title(\"LeNet5: Accuracy Curve\", fontsize = 24)\n",
    "plt.xlabel(\"Epochs\"    , fontsize = 20)\n",
    "plt.ylabel(\"Accuracy %\", fontsize = 20)\n",
    "plt.plot(train_acc_list, label = \"train acc.\")\n",
    "plt.plot(val_acc_list  , label = \"val acc.\")\n",
    "plt.legend(loc = 2, fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "## Loss\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"LeNet5: Loss Curve\", fontsize = 24)\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Probes\", fontsize = 20)\n",
    "plt.ylabel(\"Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step A6: Test LeNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Resize((height + 200, width + 200)),\n",
    "    #transforms.CenterCrop((height + 100, width + 100)),\n",
    "    #transforms.RandomCrop((height, width)),\n",
    "    #transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    #transforms.RandomVerticalFlip(p = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "Test_Dataset = AOI_Dataset(target_csv = test_csv, root_path = root_test, height = height, width = width, transform = transform_test)\n",
    "Test_DataLoader = torch.utils.data.DataLoader(dataset = Test_Dataset, batch_size = 1, shuffle = False)\n",
    "Name_of_csv_file = \"test01_0519.csv\"\n",
    "\n",
    "df_test = pd.read_csv(test_csv)\n",
    "df_test_np = df_test.to_numpy()\n",
    "\n",
    "count = -1\n",
    "with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    for datum in tqdm(Test_DataLoader):\n",
    "        count = count + 1\n",
    "        imgs = datum[0].to(device)\n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = lenet(imgs.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        df_test_np[count][1] = float(preds)\n",
    "        \n",
    "df = pd.DataFrame(df_test_np, columns = ['ID','Label'])\n",
    "df.to_csv(Name_of_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step B1: Define AlexNet.\n",
    "* Get the AlexNet from the link below.\n",
    "* github ref link: https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes: int = 6) -> None:\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "alexnet = AlexNet()\n",
    "print(alexnet.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step B2: Setup dataloader for AlexNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 127, 127]          23,296\n",
      "              ReLU-2         [-1, 64, 127, 127]               0\n",
      "         MaxPool2d-3           [-1, 64, 63, 63]               0\n",
      "            Conv2d-4          [-1, 192, 63, 63]         307,392\n",
      "              ReLU-5          [-1, 192, 63, 63]               0\n",
      "         MaxPool2d-6          [-1, 192, 31, 31]               0\n",
      "            Conv2d-7          [-1, 384, 31, 31]         663,936\n",
      "              ReLU-8          [-1, 384, 31, 31]               0\n",
      "            Conv2d-9          [-1, 256, 31, 31]         884,992\n",
      "             ReLU-10          [-1, 256, 31, 31]               0\n",
      "           Conv2d-11          [-1, 256, 31, 31]         590,080\n",
      "             ReLU-12          [-1, 256, 31, 31]               0\n",
      "        MaxPool2d-13          [-1, 256, 15, 15]               0\n",
      "AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n",
      "          Dropout-15                 [-1, 9216]               0\n",
      "           Linear-16                 [-1, 4096]      37,752,832\n",
      "             ReLU-17                 [-1, 4096]               0\n",
      "          Dropout-18                 [-1, 4096]               0\n",
      "           Linear-19                 [-1, 1024]       4,195,328\n",
      "             ReLU-20                 [-1, 1024]               0\n",
      "           Linear-21                    [-1, 6]           6,150\n",
      "          AlexNet-22                    [-1, 6]               0\n",
      "================================================================\n",
      "Total params: 44,424,006\n",
      "Trainable params: 44,424,006\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 44.55\n",
      "Params size (MB): 169.46\n",
      "Estimated Total Size (MB): 217.02\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((height + 200, width + 200)),\n",
    "    transforms.CenterCrop((height + 100, width + 100)),\n",
    "    transforms.RandomCrop((height, width)),\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    transforms.RandomVerticalFlip(p = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "if torch.cuda.is_available(): summary(alexnet, (3, height, width))\n",
    "Train_Dataset = AOI_Dataset(target_csv = train_csv, root_path = root_train, width=width, height=height, transform = transform_train)\n",
    "\n",
    "batch_size = 8\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 43\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(Train_Dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "Train_DataLoader = torch.utils.data.DataLoader(Train_Dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "Val_DataLoader = torch.utils.data.DataLoader(Train_Dataset, batch_size=batch_size, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step B3: Setup loss and hyper parameter for AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(alexnet.parameters(), lr=1e-4)\n",
    "epoch = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step B4: Train AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.687270\n",
      "[1,   200] loss: 1.673819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:08<00:00,  3.69it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 27 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:18<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 23 %\n",
      "[2,   100] loss: 1.646917\n",
      "[2,   200] loss: 1.664908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.75it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 44 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 44 %\n",
      "[3,   100] loss: 1.592413\n",
      "[3,   200] loss: 1.558945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.77it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 41 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:17<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 42 %\n",
      "[4,   100] loss: 1.460047\n",
      "[4,   200] loss: 1.380475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.75it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 31 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 34 %\n",
      "[5,   100] loss: 1.434233\n",
      "[5,   200] loss: 1.377507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.75it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 47 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 47 %\n",
      "[6,   100] loss: 1.372040\n",
      "[6,   200] loss: 1.445212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.75it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 51 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 53 %\n",
      "[7,   100] loss: 1.354175\n",
      "[7,   200] loss: 1.357860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.75it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 53 %\n",
      "[8,   100] loss: 1.247733\n",
      "[8,   200] loss: 1.322234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.75it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 40 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 40 %\n",
      "[9,   100] loss: 1.354639\n",
      "[9,   200] loss: 1.195828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.73it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 50 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:17<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 50 %\n",
      "[10,   100] loss: 1.283722\n",
      "[10,   200] loss: 1.587454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:10<00:00,  3.58it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 55 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:17<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 55 %\n",
      "[11,   100] loss: 1.288324\n",
      "[11,   200] loss: 1.159435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:10<00:00,  3.57it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 53 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:18<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 54 %\n",
      "[12,   100] loss: 1.097325\n",
      "[12,   200] loss: 1.096357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:15<00:00,  3.35it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 19 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:18<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 19 %\n",
      "[13,   100] loss: 1.036868\n",
      "[13,   200] loss: 0.965554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.74it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 71 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 70 %\n",
      "[14,   100] loss: 0.901004\n",
      "[14,   200] loss: 0.884525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:12<00:00,  3.51it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 75 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:18<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 74 %\n",
      "[15,   100] loss: 0.768765\n",
      "[15,   200] loss: 0.874061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.76it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 76 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:17<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 75 %\n",
      "[16,   100] loss: 0.805370\n",
      "[16,   200] loss: 0.628043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:08<00:00,  3.72it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 64 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 63 %\n",
      "[17,   100] loss: 0.675039\n",
      "[17,   200] loss: 0.621732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.75it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 81 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 77 %\n",
      "[18,   100] loss: 0.806049\n",
      "[18,   200] loss: 0.560303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.76it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 78 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 77 %\n",
      "[19,   100] loss: 0.587010\n",
      "[19,   200] loss: 0.658102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.75it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 80 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:17<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 79 %\n",
      "[20,   100] loss: 0.536283\n",
      "[20,   200] loss: 0.615910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.74it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 26 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 26 %\n",
      "[21,   100] loss: 0.482958\n",
      "[21,   200] loss: 0.577158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.74it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 86 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 83 %\n",
      "[22,   100] loss: 0.466601\n",
      "[22,   200] loss: 0.684196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.74it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 85 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 82 %\n",
      "[23,   100] loss: 0.944523\n",
      "[23,   200] loss: 0.591375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.73it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 81 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 79 %\n",
      "[24,   100] loss: 0.468869\n",
      "[24,   200] loss: 0.561751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:07<00:00,  3.74it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 75 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:17<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 75 %\n",
      "[25,   100] loss: 0.707820\n",
      "[25,   200] loss: 0.423272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 253/253 [01:10<00:00,  3.58it/s]\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 81 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:18<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 77 %\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "loss_list = []\n",
    "print_probe_num = 100\n",
    "for epoch in range(epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(Train_DataLoader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device) # GPU        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = alexnet(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % print_probe_num == (print_probe_num - 1):\n",
    "            print('[%d, %5d] loss: %.6f' % (epoch + 1, i + 1, running_loss / print_probe_num))\n",
    "            loss_list.append(running_loss / print_probe_num)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Train\n",
    "    with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        for datum in tqdm(Train_DataLoader):\n",
    "\n",
    "            imgs, labs = datum[0].to(device), datum[1].to(device)\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = alexnet(imgs.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labs.size(0)\n",
    "            correct += (preds == labs).sum().item()\n",
    "        train_acc_list.append(float(correct)/float(total))\n",
    "        print('Accuracy of the network on the train images: %d %%' % (100 * correct / total))\n",
    "        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Val\n",
    "    with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        for datum in tqdm(Val_DataLoader):\n",
    "\n",
    "            imgs, labs = datum[0].to(device), datum[1].to(device)\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = alexnet(imgs.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labs.size(0)\n",
    "            correct += (preds == labs).sum().item()\n",
    "        val_acc_list.append(float(correct)/float(total))\n",
    "        print('Accuracy of the network on the val images: %d %%' % (100 * correct / total))\n",
    "        \n",
    "toc = time.time()\n",
    "print(f\"Spend {round(toc - tic, 2)} (sec)\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step B5: Evaluate the performance of AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.title(\"AlexNet: Accuracy Curve\", fontsize = 24)\n",
    "plt.xlabel(\"Epochs\"    , fontsize = 20)\n",
    "plt.ylabel(\"Accuracy %\", fontsize = 20)\n",
    "plt.plot(train_acc_list, label = \"train acc.\")\n",
    "plt.plot(val_acc_list  , label = \"val acc.\")\n",
    "plt.legend(loc = 2, fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "## Loss\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"AlexNet: Loss Curve\", fontsize = 24)\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Probes\", fontsize = 20)\n",
    "plt.ylabel(\"Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step B6: Test AlexNet5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Resize((height + 200, width + 200)),\n",
    "    #transforms.CenterCrop((height + 100, width + 100)),\n",
    "    #transforms.RandomCrop((height, width)),\n",
    "    #transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    #transforms.RandomVerticalFlip(p = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "Test_Dataset = AOI_Dataset(target_csv = test_csv, root_path = root_test, height = height , width = width, transform = transform_test)\n",
    "Test_DataLoader = torch.utils.data.DataLoader(dataset = Test_Dataset, batch_size = 1, shuffle = False)\n",
    "Name_of_csv_file = \"alexnet_test01.csv\"\n",
    "\n",
    "df_test = pd.read_csv(test_csv)\n",
    "df_test_np = df_test.to_numpy()\n",
    "\n",
    "count = -1\n",
    "with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    for datum in tqdm(Test_DataLoader):\n",
    "        count = count + 1\n",
    "        imgs = datum[0].to(device)\n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = alexnet(imgs.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        df_test_np[count][1] = float(preds)\n",
    "        \n",
    "df = pd.DataFrame(df_test_np, columns = ['ID','Label'])\n",
    "df.to_csv(Name_of_csv_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1: Define ResNet18.\n",
    "* Get the ResNet18 from the link below.\n",
    "* medium ref link: https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "\n",
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "        \n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)\n",
    "\n",
    "def activation_func(activation):\n",
    "    return  nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activate = activation_func(activation)\n",
    "        self.shortcut = nn.Identity()   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "    \n",
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
    "                      stride=self.downsampling, bias=False),\n",
    "            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n",
    "    \n",
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    \"\"\"\n",
    "    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
    "        )\n",
    "\n",
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
    "        )\n",
    "        \n",
    "class ResNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet layer composed by `n` blocks stacked one after the other\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            *[block(out_channels * block.expansion, \n",
    "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "    \n",
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet encoder composed by layers with increasing features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
    "                 activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.blocks_sizes = blocks_sizes\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
    "            activation_func(activation),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
    "                        block=block,*args, **kwargs),\n",
    "            *[ResNetLayer(in_channels * block.expansion, \n",
    "                          out_channels, n=n, activation=activation, \n",
    "                          block=block, *args, **kwargs) \n",
    "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "class ResnetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
    "    correct class by using a fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
    "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "def resnet18(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
    "    return ResNet(in_channels, n_classes, block=block, deepths=[2, 2, 2, 2], *args, **kwargs)\n",
    "\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "resnet = resnet18(3, 6)\n",
    "print(resnet.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step C2: Setup dataloader for ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "\n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((height + 200, width + 200)),\n",
    "    transforms.CenterCrop((height + 100, width + 100)),\n",
    "    transforms.RandomCrop((height, width)),\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    transforms.RandomVerticalFlip(p = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "if torch.cuda.is_available(): summary(resnet, (3, height, width))\n",
    "Train_Dataset = AOI_Dataset(target_csv = train_csv, root_path = root_train, width = width, height = height, transform = trainsform_train)\n",
    "\n",
    "batch_size = 8\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 43\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(Train_Dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "Train_DataLoader = torch.utils.data.DataLoader(Train_Dataset, batch_size = batch_size, sampler = train_sampler)\n",
    "Val_DataLoader = torch.utils.data.DataLoader(Train_Dataset, batch_size = batch_size, sampler = valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step C3: Setup loss and hyper parameter for ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(resnet.parameters(), lr=1e-4)\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step C4: Train ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "loss_list = []\n",
    "print_probe_num = 100\n",
    "\n",
    "for epoch in range(epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(Train_DataLoader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device) # GPU        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % print_probe_num == (print_probe_num - 1):\n",
    "            print('[%d, %5d] loss: %.6f' % (epoch + 1, i + 1, running_loss / print_probe_num))\n",
    "            loss_list.append(running_loss / print_probe_num)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Train\n",
    "    with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        for datum in tqdm(Train_DataLoader):\n",
    "\n",
    "            imgs, labs = datum[0].to(device), datum[1].to(device)\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = resnet(imgs.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labs.size(0)\n",
    "            correct += (preds == labs).sum().item()\n",
    "        train_acc_list.append(float(correct)/float(total))\n",
    "        print('Accuracy of the network on the train images: %d %%' % (100 * correct / total))\n",
    "        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Val\n",
    "    with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        for datum in tqdm(Val_DataLoader):\n",
    "\n",
    "            imgs, labs = datum[0].to(device), datum[1].to(device)\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = resnet(imgs.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labs.size(0)\n",
    "            correct += (preds == labs).sum().item()\n",
    "        val_acc_list.append(float(correct)/float(total))\n",
    "        print('Accuracy of the network on the val images: %d %%' % (100 * correct / total))\n",
    "        \n",
    "toc = time.time()\n",
    "print(f\"Spend {round(toc - tic, 2)} (sec)\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step C5: Evaluate the performance of ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.title(\"ResNet18: Accuracy Curve\", fontsize = 24)\n",
    "plt.xlabel(\"Epochs\"    , fontsize = 20)\n",
    "plt.ylabel(\"Accuracy %\", fontsize = 20)\n",
    "plt.plot(train_acc_list, label = \"train acc.\")\n",
    "plt.plot(val_acc_list  , label = \"val acc.\")\n",
    "plt.legend(loc = 2, fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "## Loss\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"ResNet18: Loss Curve\", fontsize = 24)\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Probes\", fontsize = 20)\n",
    "plt.ylabel(\"Loss\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step C6: Test ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Resize((height + 200, width + 200)),\n",
    "    #transforms.CenterCrop((height + 100, width + 100)),\n",
    "    #transforms.RandomCrop((height, width)),\n",
    "    #transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    #transforms.RandomVerticalFlip(p = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "Test_Dataset = AOI_Dataset(target_csv = test_csv, root_path = root_test, width = width, height = height, transform = transform_test)\n",
    "Test_DataLoader = torch.utils.data.DataLoader(dataset = Test_Dataset, batch_size = 1, shuffle = False)\n",
    "Name_of_csv_file = \"test01_0519.csv\"\n",
    "\n",
    "df_test = pd.read_csv(test_csv)\n",
    "df_test_np = df_test.to_numpy()\n",
    "\n",
    "count = -1\n",
    "with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    for datum in tqdm(Test_DataLoader):\n",
    "        count = count + 1\n",
    "        imgs = datum[0].to(device)\n",
    "        # calculate outputs by running images through the network \n",
    "        outputs = resnet(imgs.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        print(preds)\n",
    "        df_test_np[count][1] = float(preds)\n",
    "        \n",
    "df = pd.DataFrame(df_test_np, columns = ['ID','Label'])\n",
    "df.to_csv(Name_of_csv_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
